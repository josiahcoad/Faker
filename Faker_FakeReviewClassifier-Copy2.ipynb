{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CSCE 670 :: Information Storage and Retrieval - Final Project Report\n",
    "\n",
    "<h1><center>Amazon Fake Reviews Classifier and Analysis</center></h1>\n",
    "<h4><center> Josiah Coad, Savinay Narendra, Sheelabhadra Dey, Chaiwei Chang, Kevin Chang</center></h4>\n",
    "Github: [Click Me](https://github.com/josiahcoad/Faker)<br>\n",
    "Data: Please refer to library folder in the repository. (Source: Amazon)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  It has become a common practice for online reviews to have a major impact on the decision of the potential customers of that product. Positive reviews can result in significant financial gains. This gives a strong incentive for fraud reviews, also commonly called opinion scamming. Opinion scamming includes fake blogs, reviews, deceptive advertising and more. Our research is accordingly focusing on product reviews based on the Amazon dataset both from University of Illinois at Chicago[1] and Professor Caverlee’s lab at Texas A&M University. Reports indicated 2-6% reviews on average are fake with up to 20% on sites such as Yelp. This leads to an unrealistic representation of places and products on the internet. Additionally, there are some fake review cases in the news for example [6].\n",
    "\n",
    "  Based on existing machine learnign techniques, we applied unsupervised clustering and observe that whether the spam/fake reviewers can be clustered together. In order to enhance the performance of learning process, we preprocessed the data to focus on suspicious group or individual because we believe that the some obivious features are similar within these groups.\n",
    "  \n",
    "  The key difficulty in determining fake reviews is that it is extremely hard for humans to identify fake reviews. In one related work, it was said to take a team of industry experts eight weeks to develop a labeled data set. We believe that a machine can do better at identifying the fake reviews by extracting the implicit information or behavior feature inside the reviews efficiently.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Initialization\n",
    "\n",
    "To initialize tha data before classifying, we categorized them into groups of members because people who write fake reviews will appear some similar behavior so that this can be a clue that we find them out through group. Afterwards, we explore eight different features on each group and utilize them in the vector space. Now that we have the vector of each group, we implement scaling and normalization to find out the top k, scores of being fake reviewer. Eventually, we feed our input space to Self-Oorganising map(SOM) machine and further observe the result of clustering, and compare the results with  different approaches such as word2vec and basic cosine similarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Group Indicator\n",
    "\n",
    "By grouping our reviewers together in hand tune features of rule-base design, hopeing to get a better result of detecting fake review. Here we defined the rules of grouping: (1) If a member rates a certain product with ONE or FIVE stars, he/she is eligible for group. (2) If eligible members commonly rate on the same three or more products, we will form them a group. Note that one member can be in different groups. For example, member A, B, C, and D rate on the same three products i, j, and k as five stars. Therefore, they will form a group. Furthermore, member A, B and C rate products i, j, k, and l as five stars, they will form another group. The condition of rules has the hueristic meaning because one that rate extreme scors may be attempting to make impact on certain product, and if all the members in a group commonly rate on certains product, they has higher probabilities to be conspiracy. By applying these rules, we obtain 1307 different groups and 82577 products that has been rated over once. The related codework is shown on two cells below.\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 99123\n",
      "Number of reviewers: 4746\n",
      "Number of reviewers with 3+ reviews rated 1 or 5 star: 3268\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# returns a list of reviews represented as dict objects.\n",
    "# put in a number of lines to read from file\n",
    "# or put in no number and it will read all\n",
    "def parserJSON(path, numLines=None):\n",
    "  numLines = numLines or len(open(path).read().split(\"\\n\")) - 1\n",
    "  with open(path) as txt:\n",
    "    reviews = [eval(next(txt)) for x in range(numLines)]\n",
    "  print(\"Number of reviews:\", len(reviews))\n",
    "  return reviews\n",
    "\n",
    "# create a dict with reviewer ID as key and a list of the reviewers reviews as the value\n",
    "def get_reviewers(reviews):\n",
    "   reviewers = {}\n",
    "   for review in reviews:\n",
    "      reviewerId = review[\"memberId\"]\n",
    "      if reviewerId not in reviewers:\n",
    "         reviewers[reviewerId] = [review]\n",
    "      else:\n",
    "         reviewers[reviewerId].append(review)\n",
    "   print(\"Number of reviewers:\", len(reviewers))\n",
    "   return reviewers\n",
    "\n",
    "# takes a list of tuples (reviewer, reviews)\n",
    "# filter out reviewers who did reviewed less than three products which have been rated 1 or 5 star\n",
    "# according to the paper, fraud reviewers will review at least three products to get their money's worth\n",
    "def remove_lessthan3(reviewers_reviews):\n",
    "   final = {}\n",
    "   for reviewer, reviews in reviewers_reviews.items():\n",
    "      reviews = list(filter(lambda review: review[\"Rate\"] == 1 or review[\"Rate\"] == 5, reviews))\n",
    "      if len(reviews) >= 3:\n",
    "            final[reviewer] = sorted(reviews, key=lambda review: review[\"productId\"])\n",
    "   print(\"Number of reviewers with 3+ reviews rated 1 or 5 star:\", len(final))\n",
    "   return final\n",
    "\n",
    "# create a dict with product ID as the key and a list of the product's reviews as the value\n",
    "def get_products(reviews):\n",
    "   products = {}\n",
    "   for review in reviews:\n",
    "      productId = review[\"productId\"]\n",
    "      if productId not in products:\n",
    "         products[productId] = [review]\n",
    "      else:\n",
    "         products[productId].append(review)\n",
    "   return products\n",
    "\n",
    "\n",
    "def normalizedVector(vector):\n",
    "    total = 0\n",
    "    for key in vector:\n",
    "        total += vector[key] ** 2\n",
    "    total = total ** 0.5\n",
    "    for key in vector:\n",
    "        vector[key] /= total\n",
    "    return vector\n",
    "\n",
    "# takes a dictionary of groups which are organized by groupID as the key and a list of tuples as the value\n",
    "# return a list of groups where each group is structured as: [(product, [reviews]), (product, [reviews])]\n",
    "def organize_by_product(groups_dict):\n",
    "   group_list = []\n",
    "   for groupId, group in groups_dict.items():\n",
    "      reviews = []\n",
    "      for user, user_reviews in group:\n",
    "         reviews.extend(user_reviews)\n",
    "      products_reviews = defaultdict(list)\n",
    "      for review in reviews:\n",
    "         products_reviews[review[\"productId\"]].append(review)\n",
    "      group_list.append( products_reviews.items() )\n",
    "   return group_list\n",
    "\n",
    "reviewers_products = []\n",
    "\n",
    "# get a list of dictionary items which represent each review object (including metadata like product id and user id) \n",
    "reviews = parserJSON('./library/amazon-review-data-modified.json')\n",
    "# get a list of tuples with user as first entry and a list of the review objects their part of as the second\n",
    "reviewers_reviews_dict = get_reviewers(reviews)\n",
    "reviewers_reviews = reviewers_reviews_dict\n",
    "# remove all reviewers who reviewed less than 3 products with ratings other than 1 or 5\n",
    "reviewers_reviews = remove_lessthan3(reviewers_reviews)\n",
    "\n",
    "# create a new list of tuples... with first entry being the reviewer \n",
    "# and second being a list of the product ids reviewed\n",
    "for reviewer, reviews in reviewers_reviews.items():\n",
    "   reviewers_products.append( (reviewer, [review[\"productId\"] for review in reviews]) )\n",
    "\n",
    "# get a sorted list of reviews that a user left for products which match 'productIds'\n",
    "def get_product_reviews(productIds, userId):\n",
    "   return [review for review in reviewers_reviews_dict[userId] if review[\"productId\"] in productIds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The eight spam features we applied are describes as below [2]:\n",
    "\n",
    "\n",
    "<ol>\n",
    "    <li>Group Time Window (GTW):</li>\n",
    "    <p>The assumption is that the group tends to work together within a period of time, which means we will give higher scores to those rate the same product within a short time period. In our experiment, we set up the window as four days.</p>\n",
    "    \n",
    "\\begin{equation*}\n",
    "    GTW(g, p)=\n",
    "    \\begin{cases}\n",
    "      0, & \\text{if}\\ L(g, p) - F(g, p)>\\tau \\\\\n",
    "      1-\\frac{L(g, p) - F(g, p)}{\\tau}, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "    <li>Group Deviation (GD):</li>\n",
    "    <p>If one group has higher deviation with other members' rating on the certain product, they all become suspicious so that we will give them higher score in this indicator</p>\n",
    "$$D(g, p) = \\frac{r_{p,g} - \\bar{r}_{p,g}}{4}$$\n",
    "\n",
    "    <li>Group Content Similarity (GCS):</li>\n",
    "    <p>This indicatoe capture the similarity of the review text within the group member. Because the fake review may duplicate the review from their member or make a very smilar one.</p>\n",
    "$$CS_G(g,p)=avg_{m_i,m_j\\in g,i < j} \\Big( cosine(c(m_i,p),c(m_j,p) \\Big)$$\n",
    "\n",
    "    <li>Group Member Content Similarity (GMCS):</li>\n",
    "    <p>This feature is similar to the previous one, while this one focuses on the member to similar products. Due to the taxing work of writing a new review, spammer tend to modified the existing fake review and paste on the similar products.</p>\n",
    "$$CS_M(g,m)=avg_{p_i,p_j\\in P_g,i < j} \\Big( cosine(c(m,p_i),c(m,p_j) \\Big)$$\n",
    "\n",
    "\n",
    "    <li>Group Early Time Frame (GETF):</li>\n",
    "    <p>This indicator features the importance of rating time on products, i.e. if one rates a product at the first beginging, it is more suspicious to be fake because it seems to make impact on a product when it has a few rating.</p>\n",
    "\\begin{equation*}\n",
    "    GTF(g, p)=\n",
    "    \\begin{cases}\n",
    "      0, & \\text{if}\\ L(g, p) - A(p)>\\beta \\\\\n",
    "      1-\\frac{L(g, p) - A(p)}{\\beta}, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "    <li>Group Size Ratio (GSR):</li>\n",
    "    <p>This features the ratio of the number of members in a group to the total number of reviews for a certain product. Gigher group ratio means the group dominates the rating and is more likely to be spammer.</p>\n",
    "$$GSR_p(g,p)= \\frac{|g|}{|M_p|}$$\n",
    "\n",
    "    <li>Group Size (GS):</li>\n",
    "    <p>For larger groups, members is less likely to be together by chance so that this is a simple intuitive feature.</p>\n",
    "$$GS(g)= \\frac{|g|}{max(|P_{gi}|)}$$\n",
    "\n",
    "    <li>Group Support Count (GSUP):</li>\n",
    "    <p>This indicator is the total number of products towards which the group has worked together. Group with higher support count will be given a higher score since they are likely to be spammer. Note that the score is normalized to [0,1].</p>\n",
    "$$GSUP(g)= \\frac{|P_g|}{max(|P_{gi}|)}$$\n",
    "\n",
    "Note that each value x attained by a feature $f(0 ≤ x ≤ 1)$ as $f \\in [0,1]$.\n",
    "    \n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups:  261\n"
     ]
    }
   ],
   "source": [
    "groups = []\n",
    "for i in range(len(reviewers_products)-1):\n",
    "   ref_user = reviewers_products[i]\n",
    "   newgroup = [ref_user]\n",
    "   for j in range(i+1, len(reviewers_products)):\n",
    "      compare_user = reviewers_products[j]\n",
    "      shared_products = set(ref_user[1]).intersection(set(compare_user[1]))\n",
    "      if len(shared_products) >= 3:\n",
    "         newgroup.append(compare_user)\n",
    "   if len(newgroup) >= 2:\n",
    "      group_products = sorted(list(set(ref_user[1]).intersection(*[set(user[1]) for user in newgroup])))\n",
    "      newgroup = [( user[0], get_product_reviews(group_products, user[0]) ) for user in newgroup]\n",
    "      groups.append(newgroup)\n",
    "#print(*groups[0], sep=\"\\n\\n\")\n",
    "print(\"Number of groups: \", len(groups))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 99123\n",
      "Number of reviewers: 4746\n",
      "Number of reviewers with 3+ reviews rated 1 or 5 star: 3268\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "reviews = parserJSON('./library/amazon-review-data-modified.json')\n",
    "# get a dict with {user : sorted list (by productId) of their review objects, ...}\n",
    "user_dict = get_reviewers(reviews)\n",
    "# remove all reviewers who reviewed less than 3 products with ratings other than 1 or 5\n",
    "user_dict = remove_lessthan3(user_dict)\n",
    "\n",
    "\n",
    "# takes a dictionary of users and their products\n",
    "# returns a dictionary of {group1 : [(userId1, [{R1}, {R2}, ...]), (userId2, [{R1}, {R2}, ...]) ...], ...}\n",
    "# groups are represented as concatenated productId's of products in that group: \"R1-R2-R3\"\n",
    "def group_users(users_dict):\n",
    "   # get a tuple of... (memberId, list of reviews that a user left for products which match 'productIds')\n",
    "   def get_entry(memberId, productId_list):\n",
    "      reviews = [review for review in users_dict[memberId] if review[\"productId\"] in productId_list]\n",
    "      return ( memberId, reviews )\n",
    "   # create a list of tuples of... (reviewer, list of the product ids reviewed)\n",
    "   users = [(memberId, [(review[\"productId\"], review[\"Rate\"]) for review in reviews]) for memberId, reviews in users_dict.items()]# equal to the function of nested loop\n",
    "   groups = {}\n",
    "   count = 0\n",
    "   for i in range(len(users)-1):\n",
    "      ref_user = users[i]\n",
    "      for j in range(i+1,len(users)):\n",
    "         comp_user = users[j]\n",
    "         common_products = set(ref_user[1]).intersection(set(comp_user[1]))#cmp current and it's next till end\n",
    "         common_products = sorted([review[0] for review in common_products])#sort by\n",
    "         if len(common_products) >= 3:\n",
    "            key = count##intersection\n",
    "            comp_entry = get_entry(comp_user[0], common_products)\n",
    "            if key in groups:\n",
    "               if comp_user[0] not in [entry[0] for entry in groups[key]]: # make sure user's not already in group\n",
    "                  groups[key].append(comp_entry)\n",
    "            else:\n",
    "               ref_entry  = get_entry(ref_user[0] , common_products)\n",
    "               groups[key] = [ref_entry, comp_entry]\n",
    "            count += 1\n",
    "\n",
    "   return groups\n",
    "\n",
    "with open(\"./library/groups_temp.txt\", \"w\") as f:\n",
    "   final_user_dict = collections.OrderedDict(sorted(group_users(user_dict).items()))\n",
    "   f.write(repr(final_user_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 99123\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "from cosine_sim import cosine_sim\n",
    "from numpy import mean as avg\n",
    "from modules.amazon_parser import *\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "MAX_USERS_IN_GROUP = 5 # found previously\n",
    "MAX_PRODS_IN_GROUP = 7 # found previously\n",
    "SIX_MONTHS = 15552000 # seconds in 6 months, used in GETF\n",
    "FOUR_DAYS = 345600 # number of seconds in 4 days\n",
    "\n",
    "\n",
    "\n",
    "review_objects = parserJSON('./library/amazon-review-data-modified.json')\n",
    "\n",
    "products_dict  = get_products(review_objects) # create a dict with product ID as the key and a list of the product's reviews as the value\n",
    "\n",
    "\n",
    "with open(\"./library/groups_temp.txt\") as f:\n",
    "   groups = eval(f.read())\n",
    "\n",
    "groups_by_products = organize_by_product(groups)\n",
    "groups_by_reviewers = organize_by_user(groups)\n",
    "\n",
    "# Group Deviation (GD)\n",
    "def GD(group_by_products):\n",
    "  return max([D(product, reviews) for product, reviews in group_by_products])\n",
    "\n",
    "def D(product, reviews):\n",
    "  group_prod_rate = reviews[0][\"Rate\"]\n",
    "  avg_prod_rate = avg([review[\"Rate\"] for review in products_dict[product]])\n",
    "  return abs(group_prod_rate - avg_prod_rate) / 4.0\n",
    "\n",
    "# Group Member Content Similarity (GMCS)\n",
    "def GMCS(groups_by_reviewers):\n",
    "  return sum([MS(reviews) for reviewer, reviews in groups_by_reviewers]) / len(groups_by_reviewers)\n",
    "\n",
    "def MS(reviews):\n",
    "  texts = [review[\"reviewText\"] for review in reviews]\n",
    "  return avg([cosine_sim(review1, review2) for review1 in texts for review2 in texts])\n",
    "\n",
    "# Group Size (GS) (number of users in group)\n",
    "def GS(group_by_users):\n",
    "    return float(len(group_by_users)) / MAX_USERS_IN_GROUP\n",
    "\n",
    "# Group Size Ratio (GSR) (returns 1 if each product in the group were only reviewed by the group members)\n",
    "def GSR(group_by_products):\n",
    "  return avg ( [gsr(product, reviews) for product, reviews in group_by_products] )\n",
    "\n",
    "def gsr(product, reviews):\n",
    "  return float(len(reviews)) / len(products_dict[product])\n",
    "# ------------------------\n",
    "\n",
    "def GTW(group):\n",
    "   return max([prod_TW(reviews) for product, reviews in group])\n",
    "\n",
    "def prod_TW(reviews):\n",
    "   timestamps = [float(review[\"Date\"]) for review in reviews]\n",
    "   _range = max(timestamps)-min(timestamps)\n",
    "   return 1-_range/FOUR_DAYS if _range <= FOUR_DAYS else 0\n",
    "\n",
    "def GCS(group):\n",
    "   return max([CS(reviews) for product, reviews in group])\n",
    "\n",
    "def CS(reviews):\n",
    "   texts = [review[\"reviewText\"] for review in reviews]\n",
    "   return avg([cosine_sim(review1, review2) for review1 in texts for review2 in texts])\n",
    "\n",
    "def GETF(group):\n",
    "   return max([GTF(product, reviews) for product, reviews in group])\n",
    "\n",
    "def GTF(product, reviews):\n",
    "   earliest_product_review = min([float(review[\"Date\"]) for review in products_dict[product]])\n",
    "   latest_group_review = max([float(review[\"Date\"]) for review in reviews])\n",
    "   _range = latest_group_review-earliest_product_review\n",
    "   return 1-_range/SIX_MONTHS if _range <= SIX_MONTHS else 0\n",
    "\n",
    "# Group Support Count (GSUP) (number of products in group)\n",
    "def GSUP(group):\n",
    "  return float(len(group)) / MAX_PRODS_IN_GROUP\n",
    "\n",
    "# Sum Scores\n",
    "def scores(gbp, gbr):\n",
    "   return [GCS(gbp), GTW(gbp), GETF(gbp), GSUP(gbp), GS(gbr), GSR(gbp), GD(gbp), GMCS(gbr)]\n",
    "\n",
    "\n",
    "def get_all_scores():\n",
    "  all_scores = []\n",
    "  for i in range(len(groups_by_reviewers)):\n",
    "     l = [i] + scores(groups_by_products[i], groups_by_reviewers[i])\n",
    "     all_scores.append(l)\n",
    "  return all_scores\n",
    "\n",
    "with open(\"./library/groups_temp.txt\") as f:\n",
    "    groups = eval(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Individual Indicator\n",
    "\n",
    "Moreover, we should also find some of the features from individual. Although group behaviors are important, they hide a lot of details about its members. Clearly, individual members’ behaviors also give signals for group spamming. We get some indicators as following code, and append to our feature list to become latter traning input.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering\n",
    "### Self-Organizing Map\n",
    "A Self-Organizing Map, also called Kohonen Map, is a type of neural network which maps points from an input space to points in an output space. This transformation keeps the topology of the data by using a set of neurons in the same feature space fitted to the dataset so that the final topology of the neural network is a good representation of the data. By doing so, points that were close in the input space will also be close in the output space.\n",
    "For an input data point in the high-dimensional feature space, the corresponding output will be the neuron in the feature space which is the closest to this point.\n",
    "\n",
    "<img src=\"./images/fig_1.png\" />\n",
    "\n",
    "### Training\n",
    "A self-organizing map consists of components called nodes or neurons. Associated with each node are a weight vector of the same dimension as the input data vectors, and a position in the map space. At first, we assign the weights of the neurons be initialized to small random value. Than, the neuron whose weight vector is most similar to the random sampling input is called the best matching unit(BMU). The weights of the BMU and neurons close to it in the SOM lattice are adjusted towards the input vector. During mapping, there will be one single winning neuron: the neuron whose weight vector lies closest to the input vector. This can be simply determined by calculating the Euclidean distance between input vector and weight vector. By the same time, we also up date the neighbors that fulfill neurons cooperation that near-by neurons on the lattice get a chance to adapt.\n",
    "Our updating weight function is $$\\vec{w}_j :: \\vec{w}_j + \\eta h(j,i(x))(\\vec{x}-\\vec{w}_j) $$ where $$i(x) = argmin_j|\\vec{x}-\\vec{w}_j|$$\n",
    "The figures below show examples of SOM structures with an input layer in a 6-dimensional feature space (on the left) and two possible network topologies (on the right). The only connections shown are between the input vector and one output neuron, but each dimension of the input layer is actually connected to every node.\n",
    "\n",
    "<img src=\"./images/fig_2.png\" />\n",
    "\n",
    "\n",
    "\n",
    "#### Median Interneuron Distance(MID) matrix\n",
    "\n",
    "One of the advantages of SOM is to perform dimensionality reduction. However, if our data can be mapped to a 1 or 2-dimensional network as previously described, we now have to visualize this network. This can be done by computing the median interneuron distance matrix. For a 2-dimensional grid network of $MxN$ neurons, each value of the $MxN$ matrix is the median of the Euclidean distance between a neuron $w_{i,j}$ and the neurons in its neighborhood, with $1 ≤ i ≤ M$ and $1 ≤ j ≤ N$. After normalization, we obtain a weight matrix that can be plotted in 2D space. Each value of this matrix corresponds to a neuron in the network, and values close to 1 show neurons far from their neighborhood.\n",
    "\n",
    "Once the SOM is trained and the MID matrix computed, outliers can be detected. This detection starts by identifying outlying neurons, which are neurons lying far from the other neurons and could have been attracted by dense sets of outliers such as in figure (on the left) below. If such neurons exist, they can be easily identified using the MID matrix. As we can see in figure (on the right) below, outlying neurons have a value in the MID matrix much higher than the one of the other neurons. We can thus use a simple threshold or compare those distances to do our selection. The plot of the MID matrix is a good tool to check the existence of outlying neurons. When this is done, outliers are the data points having for winning node an outlying neuron.\n",
    "\n",
    "<img src=\"./images/fig_3.png\" />\n",
    "\n",
    "\n",
    "\n",
    "The figures below show the training of the SOM. We have used a 2-dimensional grid of size 10x10, each step updates the value of the winning node and its neighborhood. 400 000 iterations (equivalent to 20 times the size of the dataset) were used with an initial learning rate of 0.01 and an initial standard deviation for the neighborhood of $\\sigma = 1$. The 2D representation of the neural network has been computed by the multidimensional scaling (MDS) algorithm implemented in scikit-learn.\n",
    "\n",
    "<img src=\"./images/fig_5.png\" />\n",
    "\n",
    "\n",
    "The MID matrix of the trained network is plotted in the figure below. It was computed using the median of the distance with the 8 neighbors of each neuron. Neurons represented by values close to 1 are far from their neighborhood and will be flagged as outlying neurons.\n",
    "\n",
    "<img src=\"./images/map.png\" />\n",
    "\n",
    "\n",
    "\n",
    "### SOM for detecting fake reviewers\n",
    "\n",
    "SOM is a very good algorithm for fraud detection. It identifies data points that are not consistent with their neighbourhood. Frauds are identified by the fact that the data points corresponding to them are outliers.\n",
    "\n",
    "We believe that SOM can also work well in identifying fake online reviewers. This is because fake reviewers do not behave like normal reviewers and the data points corresponding to them are very different from the data points corresponding to normal reviewers. This problem is just like the fraud detection problem and hence a SOM would be good choice.\n",
    "\n",
    "The input to the SOM is a feature vector that contains the group features extracted from the groups and the group ID appended at the beginning of the vector. The SOM returns the map of winning neurons i.e. the weights of the neurons. The winning neurons and the median distance of the neighboring data points are projected into the MID matrix. A threshold value of 0.8 is set and elements with MID values greater than the threshold are flagged as fake reviewer groups. The reviewers belonging to the fake groups are the fake reviewers.\n",
    "A self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional, discretized representation of the input space of the training samples, called a map. In our case, after the exprirement, we decided to set our map into 2D. This dimensionality reduction makes SOMs useful for visualizing low-dimensional views of high-dimensional data. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning such as backpropagation with gradient descent, and in the sense that SOM use a neighborhood function to preserve the topological properties of the input space. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "from numpy import (array, unravel_index, nditer, linalg, random, subtract,\n",
    "                   power, exp, pi, zeros, arange, outer, meshgrid, dot)\n",
    "from collections import defaultdict\n",
    "from warnings import warn\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Minimalistic implementation of the Self Organizing Maps (SOM).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def fast_norm(x):\n",
    "    \"\"\"Returns norm-2 of a 1-D numpy array.\n",
    "\n",
    "    * faster than linalg.norm in case of 1-D arrays (numpy 1.9.2rc1).\n",
    "    \"\"\"\n",
    "    return sqrt(dot(x, x.T))\n",
    "\n",
    "\n",
    "class MiniSom(object):\n",
    "    def __init__(self, x, y, input_len, sigma=1.0, learning_rate=0.5, decay_function=None, random_seed=None):\n",
    "        \"\"\"\n",
    "            Initializes a Self Organizing Maps.\n",
    "            x,y - dimensions of the SOM\n",
    "            input_len - number of the elements of the vectors in input\n",
    "            sigma - spread of the neighborhood function (Gaussian), needs to be adequate to the dimensions of the map.\n",
    "            (at the iteration t we have sigma(t) = sigma / (1 + t/T) where T is #num_iteration/2)\n",
    "            learning_rate - initial learning rate\n",
    "            (at the iteration t we have learning_rate(t) = learning_rate / (1 + t/T) where T is #num_iteration/2)\n",
    "            decay_function, function that reduces learning_rate and sigma at each iteration\n",
    "                            default function: lambda x,current_iteration,max_iter: x/(1+current_iteration/max_iter)\n",
    "            random_seed, random seed to use.\n",
    "        \"\"\"\n",
    "        if sigma >= x/2.0 or sigma >= y/2.0:\n",
    "            warn('Warning: sigma is too high for the dimension of the map.')\n",
    "        if random_seed:\n",
    "            self.random_generator = random.RandomState(random_seed)\n",
    "        else:\n",
    "            self.random_generator = random.RandomState(random_seed)\n",
    "        if decay_function:\n",
    "            self._decay_function = decay_function\n",
    "        else:\n",
    "            self._decay_function = lambda x, t, max_iter: x/(1+t/max_iter)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.sigma = sigma\n",
    "        self.weights = self.random_generator.rand(x,y,input_len)*2-1 # random initialization\n",
    "        for i in range(x):\n",
    "            for j in range(y):\n",
    "                self.weights[i,j] = self.weights[i,j] / fast_norm(self.weights[i,j]) # normalization\n",
    "        self.activation_map = zeros((x,y))\n",
    "        self.neigx = arange(x)\n",
    "        self.neigy = arange(y) # used to evaluate the neighborhood function\n",
    "        self.neighborhood = self.gaussian\n",
    "\n",
    "    def _activate(self, x):\n",
    "        \"\"\" Updates matrix activation_map, in this matrix the element i,j is the response of the neuron i,j to x \"\"\"\n",
    "        s = subtract(x, self.weights) # x - w\n",
    "        it = nditer(self.activation_map, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            self.activation_map[it.multi_index] = fast_norm(s[it.multi_index])  # || x - w ||\n",
    "            it.iternext()\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\" Returns the activation map to x \"\"\"\n",
    "        self._activate(x)\n",
    "        return self.activation_map\n",
    "\n",
    "    def gaussian(self, c, sigma):\n",
    "        \"\"\" Returns a Gaussian centered in c \"\"\"\n",
    "        d = 2*pi*sigma*sigma\n",
    "        ax = exp(-power(self.neigx-c[0], 2)/d)\n",
    "        ay = exp(-power(self.neigy-c[1], 2)/d)\n",
    "        return outer(ax, ay)  # the external product gives a matrix\n",
    "\n",
    "    def diff_gaussian(self, c, sigma):\n",
    "        \"\"\" Mexican hat centered in c (unused) \"\"\"\n",
    "        xx, yy = meshgrid(self.neigx, self.neigy)\n",
    "        p = power(xx-c[0], 2) + power(yy-c[1], 2)\n",
    "        d = 2*pi*sigma*sigma\n",
    "        return exp(-p/d)*(1-2/d*p)\n",
    "\n",
    "    def winner(self, x):\n",
    "        \"\"\" Computes the coordinates of the winning neuron for the sample x \"\"\"\n",
    "        self._activate(x)\n",
    "        return unravel_index(self.activation_map.argmin(), self.activation_map.shape)\n",
    "\n",
    "    def update(self, x, win, t):\n",
    "        \"\"\"\n",
    "            Updates the weights of the neurons.\n",
    "            x - current pattern to learn\n",
    "            win - position of the winning neuron for x (array or tuple).\n",
    "            t - iteration index\n",
    "        \"\"\"\n",
    "        eta = self._decay_function(self.learning_rate, t, self.T)\n",
    "        sig = self._decay_function(self.sigma, t, self.T) # sigma and learning rate decrease with the same rule\n",
    "        g = self.neighborhood(win, sig)*eta # improves the performances\n",
    "        it = nditer(g, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            # eta * neighborhood_function * (x-w)\n",
    "            self.weights[it.multi_index] += g[it.multi_index]*(x-self.weights[it.multi_index])\n",
    "            # normalization\n",
    "            self.weights[it.multi_index] = self.weights[it.multi_index] / fast_norm(self.weights[it.multi_index])\n",
    "            it.iternext()\n",
    "\n",
    "    def quantization(self, data):\n",
    "        \"\"\" Assigns a code book (weights vector of the winning neuron) to each sample in data. \"\"\"\n",
    "        q = zeros(data.shape)\n",
    "        for i, x in enumerate(data):\n",
    "            q[i] = self.weights[self.winner(x)]\n",
    "        return q\n",
    "\n",
    "    def random_weights_init(self, data):\n",
    "        \"\"\" Initializes the weights of the SOM picking random samples from data \"\"\"\n",
    "        it = nditer(self.activation_map, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            self.weights[it.multi_index] = data[self.random_generator.randint(len(data))]\n",
    "            self.weights[it.multi_index] = self.weights[it.multi_index]/fast_norm(self.weights[it.multi_index])\n",
    "            it.iternext()\n",
    "\n",
    "    def train_random(self, data, num_iteration):\n",
    "        \"\"\" Trains the SOM picking samples at random from data \"\"\"\n",
    "        self._init_T(num_iteration)\n",
    "        for iteration in range(num_iteration):\n",
    "            rand_i = self.random_generator.randint(len(data)) # pick a random sample\n",
    "            self.update(data[rand_i], self.winner(data[rand_i]), iteration)\n",
    "\n",
    "    def train_batch(self, data, num_iteration):\n",
    "        \"\"\" Trains using all the vectors in data sequentially \"\"\"\n",
    "        self._init_T(len(data)*num_iteration)\n",
    "        iteration = 0\n",
    "        while iteration < num_iteration:\n",
    "            idx = iteration % (len(data)-1)\n",
    "            self.update(data[idx], self.winner(data[idx]), iteration)\n",
    "            iteration += 1\n",
    "\n",
    "    def _init_T(self, num_iteration):\n",
    "        \"\"\" Initializes the parameter T needed to adjust the learning rate \"\"\"\n",
    "        self.T = num_iteration/2  # keeps the learning rate nearly constant for the last half of the iterations\n",
    "\n",
    "    def distance_map(self):\n",
    "        \"\"\" Returns the distance map of the weights.\n",
    "            Each cell is the normalised sum of the distances between a neuron and its neighbours.\n",
    "        \"\"\"\n",
    "        um = zeros((self.weights.shape[0], self.weights.shape[1]))\n",
    "        it = nditer(um, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            for ii in range(it.multi_index[0]-1, it.multi_index[0]+2):\n",
    "                for jj in range(it.multi_index[1]-1, it.multi_index[1]+2):\n",
    "                    if ii >= 0 and ii < self.weights.shape[0] and jj >= 0 and jj < self.weights.shape[1]:\n",
    "                        um[it.multi_index] += fast_norm(self.weights[ii, jj, :]-self.weights[it.multi_index])\n",
    "            it.iternext()\n",
    "        um = um/um.max()\n",
    "        return um\n",
    "\n",
    "    def activation_response(self, data):\n",
    "        \"\"\"\n",
    "            Returns a matrix where the element i,j is the number of times\n",
    "            that the neuron i,j have been winner.\n",
    "        \"\"\"\n",
    "        a = zeros((self.weights.shape[0], self.weights.shape[1]))\n",
    "        for x in data:\n",
    "            a[self.winner(x)] += 1\n",
    "        return a\n",
    "\n",
    "    def quantization_error(self, data):\n",
    "        \"\"\"\n",
    "            Returns the quantization error computed as the average distance between\n",
    "            each input sample and its best matching unit.\n",
    "        \"\"\"\n",
    "        error = 0\n",
    "        for x in data:\n",
    "            error += fast_norm(x-self.weights[self.winner(x)])\n",
    "        return error/len(data)\n",
    "\n",
    "    def win_map(self, data):\n",
    "        \"\"\"\n",
    "            Returns a dictionary wm where wm[(i,j)] is a list with all the patterns\n",
    "            that have been mapped in the position i,j.\n",
    "        \"\"\"\n",
    "        winmap = defaultdict(list)\n",
    "        for x in data:\n",
    "            winmap[self.winner(x)].append(x)\n",
    "        return winmap\n",
    "\n",
    "### unit tests\n",
    "from numpy.testing import assert_almost_equal, assert_array_almost_equal, assert_array_equal\n",
    "\n",
    "\n",
    "class TestMinisom:\n",
    "    def setup_method(self, method):\n",
    "        self.som = MiniSom(5, 5, 1)\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                assert_almost_equal(1.0, linalg.norm(self.som.weights[i,j]))  # checking weights normalization\n",
    "        self.som.weights = zeros((5, 5))  # fake weights\n",
    "        self.som.weights[2, 3] = 5.0\n",
    "        self.som.weights[1, 1] = 2.0\n",
    "\n",
    "    def test_decay_function(self):\n",
    "        assert self.som._decay_function(1., 2., 3.) == 1./(1.+2./3.)\n",
    "\n",
    "    def test_fast_norm(self):\n",
    "        assert fast_norm(array([1, 3])) == sqrt(1+9)\n",
    "\n",
    "    def test_gaussian(self):\n",
    "        bell = self.som.gaussian((2, 2), 1)\n",
    "        assert bell.max() == 1.0\n",
    "        assert bell.argmax() == 12  # unravel(12) = (2,2)\n",
    "\n",
    "    def test_win_map(self):\n",
    "        winners = self.som.win_map([5.0, 2.0])\n",
    "        assert winners[(2, 3)][0] == 5.0\n",
    "        assert winners[(1, 1)][0] == 2.0\n",
    "\n",
    "    def test_activation_reponse(self):\n",
    "        response = self.som.activation_response([5.0, 2.0])\n",
    "        assert response[2, 3] == 1\n",
    "        assert response[1, 1] == 1\n",
    "\n",
    "    def test_activate(self):\n",
    "        assert self.som.activate(5.0).argmin() == 13.0  # unravel(13) = (2,3)\n",
    "\n",
    "    def test_quantization_error(self):\n",
    "        self.som.quantization_error([5, 2]) == 0.0\n",
    "        self.som.quantization_error([4, 1]) == 0.5\n",
    "\n",
    "    def test_quantization(self):\n",
    "        q = self.som.quantization(array([4, 2]))\n",
    "        assert q[0] == 5.0\n",
    "        assert q[1] == 2.0\n",
    "\n",
    "    def test_random_seed(self):\n",
    "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        assert_array_almost_equal(som1.weights, som2.weights)  # same initialization\n",
    "        data = random.rand(100,2)\n",
    "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som1.train_random(data,10)\n",
    "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som2.train_random(data,10)\n",
    "        assert_array_almost_equal(som1.weights,som2.weights)  # same state after training\n",
    "\n",
    "    def test_train_batch(self):\n",
    "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        data = array([[4, 2], [3, 1]])\n",
    "        q1 = som.quantization_error(data)\n",
    "        som.train_batch(data, 10)\n",
    "        assert q1 > som.quantization_error(data)\n",
    "\n",
    "    def test_train_random(self):\n",
    "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        data = array([[4, 2], [3, 1]])\n",
    "        q1 = som.quantization_error(data)\n",
    "        som.train_random(data, 10)\n",
    "        assert q1 > som.quantization_error(data)\n",
    "\n",
    "    def test_random_weights_init(self):\n",
    "        som = MiniSom(2, 2, 2, random_seed=1)\n",
    "        som.random_weights_init(array([[1.0, .0]]))\n",
    "        for w in som.weights:\n",
    "            assert_array_equal(w[0], array([1.0, .0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#from SOM_Trianing import som\n",
    "import numpy as np\n",
    "from group_analysis import *\n",
    "\n",
    "from grouping import group_users\n",
    "from modules.amazon_parser import *\n",
    "\n",
    "\n",
    "X = np.array(get_all_scores())\n",
    "\n",
    "# Training the SOM\n",
    "# We will use MiniSom 1.0\n",
    "from minisom import MiniSom\n",
    "som = MiniSom(x = 10, y = 10, input_len = 9, sigma = 1.0, learning_rate = 0.5)\n",
    "som.random_weights_init(X)\n",
    "som.train_random(data = X, num_iteration = 10)\n",
    "\n",
    "\n",
    "som_distance = som.distance_map().T\n",
    "#print('som map', som_distance)\n",
    "\n",
    "# Detecting the fake groups and reviewers\n",
    "mappings = som.win_map(X)\n",
    "print('size of the SOM map', len(mappings))\n",
    "text_file = open(\"mappings.txt\", \"w\")\n",
    "text_file.write(\"winning map is %s\" % mappings)\n",
    "text_file.close()\n",
    "\n",
    "# List of fake review group IDs\n",
    "faker_list = []\n",
    "for i in xrange(len(som_distance)):\n",
    "\tfor j in xrange(len(som_distance[0])):\n",
    "\t\tif som_distance[i][j] < 0.1:\n",
    "\t\t\tfake_groups = mappings[(i,j)]\n",
    "\t\t\tfor group in fake_groups:\n",
    "\t\t\t\tfake_group_ID = group[0]\n",
    "\t\t\t\tfaker_list.append(fake_group_ID)\n",
    "print('list of fake group IDs is\\n', faker_list)\n",
    "\n",
    "for faker in faker_list:\n",
    "\tprint(groups[faker])\n",
    "\n",
    "# Visualizing the results (SOM in a 2-D plot)\n",
    "from pylab import bone, pcolor, colorbar, plot, show\n",
    "bone() # creates a white window\n",
    "pcolor(som.distance_map().T)\n",
    "colorbar()\n",
    "markers = ['o', 's']\n",
    "colors = ['r', 'g']\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fake_user_list = []\n",
    "for faker in faker_list:\n",
    "    for review in groups[faker]:\n",
    "        fake_user_list.append(review[0])\n",
    "\n",
    "print(\"No. of fake reviewers: \", len(fake_user_list))\n",
    "print(\"List of faker reviewers: \", fake_user_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Spam Detection using Reviewer Centric Features\n",
    "\n",
    "Identifying spammers can improve detection of fake reviews, since many spammers share profile characteristics and activity patterns. We have studied various combinations of features engineered from reviewer profile characteristics and behavioral patterns. \n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def parserJSON(path, numLines=None):\n",
    "    numLines = numLines or len(open(path).read().split(\"\\n\")) - 1\n",
    "    with open(path) as txt:\n",
    "        reviews = [eval(next(txt)) for x in range(numLines)]\n",
    "    return reviews\n",
    "\n",
    "reviews = parserJSON('./library/amazon-review-data-modified.json')\n",
    "\n",
    "reviewer_collection = defaultdict(lambda: defaultdict(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Maximum number of reviews\n",
    "\n",
    "It was observed that about 75 % of spammers write more than 5 reviews on any given day. Therefore, taking into account the number of reviews a user writes per day can help detect spammers since 90 % of legitimate reviewers never create more than one review on any given day. \n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the average no. of reviews given by a user per day\n",
    "\n",
    "def maximum_reviews(reviews):\n",
    "    reviews_per_day = defaultdict(lambda: defaultdict(float))\n",
    "    avg_reviews_per_day = defaultdict(lambda: defaultdict(float))\n",
    "    for i in range(len(reviews)):\n",
    "        day = time.strftime('%Y%m%d', time.localtime(float(reviews[i][\"Date\"])))\n",
    "        reviews_per_day[reviews[i][\"memberId\"]][day] += 1\n",
    "\n",
    "    for key1 in reviews_per_day:\n",
    "        for key2 in reviews_per_day[key1]:\n",
    "            avg_reviews_per_day[key1] =  sum(reviews_per_day[key1].values())/len(reviews_per_day[key1].keys())\n",
    "    count = 0\n",
    "    for key in avg_reviews_per_day:\n",
    "        if avg_reviews_per_day[key] > 3:\n",
    "            count += 1\n",
    "    return avg_reviews_per_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Percentage of positive reviews\n",
    "\n",
    "Approximately 85 % of spammers wrote more than 80 % of their reviews as positive reviews, thus a high percentage of positive reviews might be an indication of an un- trustworthy reviewer. \n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate the % of positive reviews given by a user\n",
    "\n",
    "def pos_reviews(reviews):\n",
    "    sentiment_reviews = defaultdict(lambda: defaultdict(float))\n",
    "    for i in range(len(reviews)):\n",
    "        sentiment_reviews[reviews[i][\"memberId\"]][reviews[i][\"reviewId\"]] = reviews[i][\"Rate\"]\n",
    "    positive_reviews = defaultdict(lambda: float)\n",
    "\n",
    "    for key1 in sentiment_reviews:\n",
    "        count_pos = 0\n",
    "        for key2 in sentiment_reviews[key1]:\n",
    "            if sentiment_reviews[key1][key2] > 3:\n",
    "                count_pos += 1\n",
    "        positive_reviews[key1] = float(count_pos)/float(len(sentiment_reviews[key1].keys()))\n",
    "    return positive_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Review Length\n",
    "\n",
    "The average review length may be an important indication of reviewers with question- able intentions since about 80 % of spammers have no reviews longer than 135 words while more than 92 % of reliable reviewers have an average review length of greater than 200 words. \n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the average review length for each reviewer\n",
    "\n",
    "def review_length(reviews):\n",
    "    review_length_collection = defaultdict(lambda: defaultdict(float))\n",
    "    for i in range(len(reviews)):\n",
    "        review_length_collection[reviews[i][\"memberId\"]][reviews[i][\"reviewId\"]] = len(reviews[i][\"reviewText\"])\n",
    "    avg_review_length_collection = defaultdict(lambda: float)\n",
    "    for key1 in review_length_collection:\n",
    "        for key2 in review_length_collection[key1]:\n",
    "            avg_review_length_collection[key1] = float(sum(review_length_collection[key1].values()))/float(len(review_length_collection[key1].keys()))\n",
    "    return avg_review_length_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Reviewer deviation\n",
    "\n",
    "It was observed that spammers’ ratings tend to deviate from the average review rating at a far higher rate than legitimate reviewers, thus identifying user rating deviations may help in detection of dishonest reviewers. \n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the rating deviation of each user from the average ratings for that product. If the rating deviation\n",
    "# is greater than -1 or 1, we mark this as a feature of a fake reviewer.\n",
    "\n",
    "def reviewer_deviation(reviews):\n",
    "    product_rating = defaultdict(lambda: defaultdict(float))\n",
    "    for i in range(len(reviews)):\n",
    "        product_rating[reviews[i][\"productId\"]][\"memberId\"] = reviews[i][\"Rate\"]\n",
    "\n",
    "    avg_product_rating = defaultdict(lambda: float)\n",
    "\n",
    "    for key1 in product_rating:\n",
    "        for key2 in product_rating[key1]:\n",
    "            avg_product_rating[key1] = float(sum(product_rating[key1].values()))/float(len(product_rating[key1].keys()))\n",
    "\n",
    "    deviation_member_rating = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    avg_deviation_member_rating = defaultdict(lambda: float)\n",
    "\n",
    "    for i in range(len(reviews)):\n",
    "        deviation_member_rating[reviews[i][\"memberId\"]][reviews[i][\"productId\"]] = reviews[i][\"Rate\"] - avg_product_rating[reviews[i][\"productId\"]]\n",
    "\n",
    "    for key1 in deviation_member_rating:\n",
    "        for key2 in deviation_member_rating[key1]:\n",
    "            avg_deviation_member_rating[key1] = float(sum(deviation_member_rating[key1].values()))/float(len(deviation_member_rating[key1].keys()))\n",
    "\n",
    "    return avg_deviation_member_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Building the Training Data\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_feature_vector():\n",
    "    avg_reviews_per_day = maximum_reviews(reviews)\n",
    "    positive_reviews = pos_reviews(reviews)\n",
    "    avg_review_length_collection = review_length(reviews)\n",
    "    avg_deviation_member_rating = reviewer_deviation(reviews)\n",
    "\n",
    "    training_data = {} \n",
    "    count = 0\n",
    "    mapping_training_id_user_id = {}\n",
    "    for key in avg_reviews_per_day:\n",
    "        if key not in training_data:\n",
    "            training_data[key] = [count]\n",
    "            mapping_training_id_user_id[count] = key\n",
    "            count += 1\n",
    "        if avg_reviews_per_day[key] > 3:\n",
    "            training_data[key].append(100 * avg_reviews_per_day[key])\n",
    "        else:\n",
    "            training_data[key].append(0.1 * avg_reviews_per_day[key])\n",
    "\n",
    "    for key in positive_reviews:\n",
    "        if positive_reviews[key] > 0.8:\n",
    "            training_data[key].append(3 * positive_reviews[key])\n",
    "        else:\n",
    "            training_data[key].append(0.2 * positive_reviews[key])\n",
    "\n",
    "    for key in avg_review_length_collection:\n",
    "        training_data[key].append( (avg_review_length_collection[key] - 135.0 ))\n",
    "\n",
    "    for key in avg_deviation_member_rating:\n",
    "        if abs(avg_deviation_member_rating[key]) > 1.0:\n",
    "            training_data[key].append(5 * avg_deviation_member_rating[key] )\n",
    "        else:\n",
    "            training_data[key].append( avg_deviation_member_rating[key]  )\n",
    "\n",
    "    return training_data, mapping_training_id_user_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clustering\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 . K-Means\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "training_data = construct_feature_vector()[0]\n",
    "final_training_data = []\n",
    "for value in training_data.values():\n",
    "\tfinal_training_data.append(value[1:])\n",
    "X = np.array(final_training_data)\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "predictions = kmeans.predict(X)\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "fake_reviewers_kmeans = []\n",
    "for i in range(len(predictions)):\n",
    "\tif final_training_data[i][0] < 3:\n",
    "\t\tpredictions[i] = 1\n",
    "\tif predictions[i] == 0:\n",
    "\t\tcount_0 += 1\n",
    "\tif predictions[i] == 1:\n",
    "\t\tcount_1 += 1\n",
    "\tif predictions[i] == 0:\n",
    "\t\tfake_reviewers_kmeans.append(training_data.keys()[i])\n",
    "\n",
    "        \n",
    "print(\"No of fake reviewers: \", len(fake_reviewers_kmeans))\n",
    "print(\"List of fake reviewers are using K-Means: \\n\\n\", fake_reviewers_kmeans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 . Self Organizing Maps\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "from numpy import (array, unravel_index, nditer, linalg, random, subtract,\n",
    "                   power, exp, pi, zeros, arange, outer, meshgrid, dot)\n",
    "from collections import defaultdict\n",
    "from warnings import warn\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Minimalistic implementation of the Self Organizing Maps (SOM).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def fast_norm(x):\n",
    "    \"\"\"Returns norm-2 of a 1-D numpy array.\n",
    "\n",
    "    * faster than linalg.norm in case of 1-D arrays (numpy 1.9.2rc1).\n",
    "    \"\"\"\n",
    "    return sqrt(dot(x, x.T))\n",
    "\n",
    "\n",
    "class MiniSom(object):\n",
    "    def __init__(self, x, y, input_len, sigma=1.0, learning_rate=0.5, decay_function=None, random_seed=None):\n",
    "        \"\"\"\n",
    "            Initializes a Self Organizing Maps.\n",
    "            x,y - dimensions of the SOM\n",
    "            input_len - number of the elements of the vectors in input\n",
    "            sigma - spread of the neighborhood function (Gaussian), needs to be adequate to the dimensions of the map.\n",
    "            (at the iteration t we have sigma(t) = sigma / (1 + t/T) where T is #num_iteration/2)\n",
    "            learning_rate - initial learning rate\n",
    "            (at the iteration t we have learning_rate(t) = learning_rate / (1 + t/T) where T is #num_iteration/2)\n",
    "            decay_function, function that reduces learning_rate and sigma at each iteration\n",
    "                            default function: lambda x,current_iteration,max_iter: x/(1+current_iteration/max_iter)\n",
    "            random_seed, random seed to use.\n",
    "        \"\"\"\n",
    "        if sigma >= x/2.0 or sigma >= y/2.0:\n",
    "            warn('Warning: sigma is too high for the dimension of the map.')\n",
    "        if random_seed:\n",
    "            self.random_generator = random.RandomState(random_seed)\n",
    "        else:\n",
    "            self.random_generator = random.RandomState(random_seed)\n",
    "        if decay_function:\n",
    "            self._decay_function = decay_function\n",
    "        else:\n",
    "            self._decay_function = lambda x, t, max_iter: x/(1+t/max_iter)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.sigma = sigma\n",
    "        self.weights = self.random_generator.rand(x,y,input_len)*2-1 # random initialization\n",
    "        for i in range(x):\n",
    "            for j in range(y):\n",
    "                self.weights[i,j] = self.weights[i,j] / fast_norm(self.weights[i,j]) # normalization\n",
    "        self.activation_map = zeros((x,y))\n",
    "        self.neigx = arange(x)\n",
    "        self.neigy = arange(y) # used to evaluate the neighborhood function\n",
    "        self.neighborhood = self.gaussian\n",
    "\n",
    "    def _activate(self, x):\n",
    "        \"\"\" Updates matrix activation_map, in this matrix the element i,j is the response of the neuron i,j to x \"\"\"\n",
    "        s = subtract(x, self.weights) # x - w\n",
    "        it = nditer(self.activation_map, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            self.activation_map[it.multi_index] = fast_norm(s[it.multi_index])  # || x - w ||\n",
    "            it.iternext()\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\" Returns the activation map to x \"\"\"\n",
    "        self._activate(x)\n",
    "        return self.activation_map\n",
    "\n",
    "    def gaussian(self, c, sigma):\n",
    "        \"\"\" Returns a Gaussian centered in c \"\"\"\n",
    "        d = 2*pi*sigma*sigma\n",
    "        ax = exp(-power(self.neigx-c[0], 2)/d)\n",
    "        ay = exp(-power(self.neigy-c[1], 2)/d)\n",
    "        return outer(ax, ay)  # the external product gives a matrix\n",
    "\n",
    "    def diff_gaussian(self, c, sigma):\n",
    "        \"\"\" Mexican hat centered in c (unused) \"\"\"\n",
    "        xx, yy = meshgrid(self.neigx, self.neigy)\n",
    "        p = power(xx-c[0], 2) + power(yy-c[1], 2)\n",
    "        d = 2*pi*sigma*sigma\n",
    "        return exp(-p/d)*(1-2/d*p)\n",
    "\n",
    "    def winner(self, x):\n",
    "        \"\"\" Computes the coordinates of the winning neuron for the sample x \"\"\"\n",
    "        self._activate(x)\n",
    "        return unravel_index(self.activation_map.argmin(), self.activation_map.shape)\n",
    "\n",
    "    def update(self, x, win, t):\n",
    "        \"\"\"\n",
    "            Updates the weights of the neurons.\n",
    "            x - current pattern to learn\n",
    "            win - position of the winning neuron for x (array or tuple).\n",
    "            t - iteration index\n",
    "        \"\"\"\n",
    "        eta = self._decay_function(self.learning_rate, t, self.T)\n",
    "        sig = self._decay_function(self.sigma, t, self.T) # sigma and learning rate decrease with the same rule\n",
    "        g = self.neighborhood(win, sig)*eta # improves the performances\n",
    "        it = nditer(g, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            # eta * neighborhood_function * (x-w)\n",
    "            self.weights[it.multi_index] += g[it.multi_index]*(x-self.weights[it.multi_index])\n",
    "            # normalization\n",
    "            self.weights[it.multi_index] = self.weights[it.multi_index] / fast_norm(self.weights[it.multi_index])\n",
    "            it.iternext()\n",
    "\n",
    "    def quantization(self, data):\n",
    "        \"\"\" Assigns a code book (weights vector of the winning neuron) to each sample in data. \"\"\"\n",
    "        q = zeros(data.shape)\n",
    "        for i, x in enumerate(data):\n",
    "            q[i] = self.weights[self.winner(x)]\n",
    "        return q\n",
    "\n",
    "    def random_weights_init(self, data):\n",
    "        \"\"\" Initializes the weights of the SOM picking random samples from data \"\"\"\n",
    "        it = nditer(self.activation_map, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            self.weights[it.multi_index] = data[self.random_generator.randint(len(data))]\n",
    "            self.weights[it.multi_index] = self.weights[it.multi_index]/fast_norm(self.weights[it.multi_index])\n",
    "            it.iternext()\n",
    "\n",
    "    def train_random(self, data, num_iteration):\n",
    "        \"\"\" Trains the SOM picking samples at random from data \"\"\"\n",
    "        self._init_T(num_iteration)\n",
    "        for iteration in range(num_iteration):\n",
    "            rand_i = self.random_generator.randint(len(data)) # pick a random sample\n",
    "            self.update(data[rand_i], self.winner(data[rand_i]), iteration)\n",
    "\n",
    "    def train_batch(self, data, num_iteration):\n",
    "        \"\"\" Trains using all the vectors in data sequentially \"\"\"\n",
    "        self._init_T(len(data)*num_iteration)\n",
    "        iteration = 0\n",
    "        while iteration < num_iteration:\n",
    "            idx = iteration % (len(data)-1)\n",
    "            self.update(data[idx], self.winner(data[idx]), iteration)\n",
    "            iteration += 1\n",
    "\n",
    "    def _init_T(self, num_iteration):\n",
    "        \"\"\" Initializes the parameter T needed to adjust the learning rate \"\"\"\n",
    "        self.T = num_iteration/2  # keeps the learning rate nearly constant for the last half of the iterations\n",
    "\n",
    "    def distance_map(self):\n",
    "        \"\"\" Returns the distance map of the weights.\n",
    "            Each cell is the normalised sum of the distances between a neuron and its neighbours.\n",
    "        \"\"\"\n",
    "        um = zeros((self.weights.shape[0], self.weights.shape[1]))\n",
    "        it = nditer(um, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            for ii in range(it.multi_index[0]-1, it.multi_index[0]+2):\n",
    "                for jj in range(it.multi_index[1]-1, it.multi_index[1]+2):\n",
    "                    if ii >= 0 and ii < self.weights.shape[0] and jj >= 0 and jj < self.weights.shape[1]:\n",
    "                        um[it.multi_index] += fast_norm(self.weights[ii, jj, :]-self.weights[it.multi_index])\n",
    "            it.iternext()\n",
    "        um = um/um.max()\n",
    "        return um\n",
    "\n",
    "    def activation_response(self, data):\n",
    "        \"\"\"\n",
    "            Returns a matrix where the element i,j is the number of times\n",
    "            that the neuron i,j have been winner.\n",
    "        \"\"\"\n",
    "        a = zeros((self.weights.shape[0], self.weights.shape[1]))\n",
    "        for x in data:\n",
    "            a[self.winner(x)] += 1\n",
    "        return a\n",
    "\n",
    "    def quantization_error(self, data):\n",
    "        \"\"\"\n",
    "            Returns the quantization error computed as the average distance between\n",
    "            each input sample and its best matching unit.\n",
    "        \"\"\"\n",
    "        error = 0\n",
    "        for x in data:\n",
    "            error += fast_norm(x-self.weights[self.winner(x)])\n",
    "        return error/len(data)\n",
    "\n",
    "    def win_map(self, data):\n",
    "        \"\"\"\n",
    "            Returns a dictionary wm where wm[(i,j)] is a list with all the patterns\n",
    "            that have been mapped in the position i,j.\n",
    "        \"\"\"\n",
    "        winmap = defaultdict(list)\n",
    "        for x in data:\n",
    "            winmap[self.winner(x)].append(x)\n",
    "        return winmap\n",
    "\n",
    "### unit tests\n",
    "from numpy.testing import assert_almost_equal, assert_array_almost_equal, assert_array_equal\n",
    "\n",
    "\n",
    "class TestMinisom:\n",
    "    def setup_method(self, method):\n",
    "        self.som = MiniSom(5, 5, 1)\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                assert_almost_equal(1.0, linalg.norm(self.som.weights[i,j]))  # checking weights normalization\n",
    "        self.som.weights = zeros((5, 5))  # fake weights\n",
    "        self.som.weights[2, 3] = 5.0\n",
    "        self.som.weights[1, 1] = 2.0\n",
    "\n",
    "    def test_decay_function(self):\n",
    "        assert self.som._decay_function(1., 2., 3.) == 1./(1.+2./3.)\n",
    "\n",
    "    def test_fast_norm(self):\n",
    "        assert fast_norm(array([1, 3])) == sqrt(1+9)\n",
    "\n",
    "    def test_gaussian(self):\n",
    "        bell = self.som.gaussian((2, 2), 1)\n",
    "        assert bell.max() == 1.0\n",
    "        assert bell.argmax() == 12  # unravel(12) = (2,2)\n",
    "\n",
    "    def test_win_map(self):\n",
    "        winners = self.som.win_map([5.0, 2.0])\n",
    "        assert winners[(2, 3)][0] == 5.0\n",
    "        assert winners[(1, 1)][0] == 2.0\n",
    "\n",
    "    def test_activation_reponse(self):\n",
    "        response = self.som.activation_response([5.0, 2.0])\n",
    "        assert response[2, 3] == 1\n",
    "        assert response[1, 1] == 1\n",
    "\n",
    "    def test_activate(self):\n",
    "        assert self.som.activate(5.0).argmin() == 13.0  # unravel(13) = (2,3)\n",
    "\n",
    "    def test_quantization_error(self):\n",
    "        self.som.quantization_error([5, 2]) == 0.0\n",
    "        self.som.quantization_error([4, 1]) == 0.5\n",
    "\n",
    "    def test_quantization(self):\n",
    "        q = self.som.quantization(array([4, 2]))\n",
    "        assert q[0] == 5.0\n",
    "        assert q[1] == 2.0\n",
    "\n",
    "    def test_random_seed(self):\n",
    "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        assert_array_almost_equal(som1.weights, som2.weights)  # same initialization\n",
    "        data = random.rand(100,2)\n",
    "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som1.train_random(data,10)\n",
    "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som2.train_random(data,10)\n",
    "        assert_array_almost_equal(som1.weights,som2.weights)  # same state after training\n",
    "\n",
    "    def test_train_batch(self):\n",
    "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        data = array([[4, 2], [3, 1]])\n",
    "        q1 = som.quantization_error(data)\n",
    "        som.train_batch(data, 10)\n",
    "        assert q1 > som.quantization_error(data)\n",
    "\n",
    "    def test_train_random(self):\n",
    "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        data = array([[4, 2], [3, 1]])\n",
    "        q1 = som.quantization_error(data)\n",
    "        som.train_random(data, 10)\n",
    "        assert q1 > som.quantization_error(data)\n",
    "\n",
    "    def test_random_weights_init(self):\n",
    "        som = MiniSom(2, 2, 2, random_seed=1)\n",
    "        som.random_weights_init(array([[1.0, .0]]))\n",
    "        for w in som.weights:\n",
    "            assert_array_equal(w[0], array([1.0, .0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "X = np.array(construct_feature_vector()[0].values())\n",
    "mapping = construct_feature_vector()[1]\n",
    "\n",
    "# Training the SOM\n",
    "# We will use MiniSom 1.0\n",
    "from minisom import MiniSom\n",
    "som = MiniSom(x = 10, y = 10, input_len = 5, sigma = 1.0, learning_rate = 0.5)\n",
    "som.random_weights_init(X)\n",
    "som.train_random(data = X, num_iteration = 100)\n",
    "\n",
    "\n",
    "som_distance = som.distance_map().T\n",
    "\n",
    "mappings = som.win_map(X)\n",
    "print('size of the SOM map', len(mappings))\n",
    "text_file = open(\"mappings_individual.txt\", \"w\")\n",
    "text_file.write(\"winning map is %s\" % mappings)\n",
    "text_file.close()\n",
    "\n",
    "# List of fake review user IDs\n",
    "faker_list = []\n",
    "faker_id = []\n",
    "for i in xrange(len(som_distance)):\n",
    "\tfor j in xrange(len(som_distance[0])):\n",
    "\t\tif som_distance[i][j] > 0.8:\n",
    "\t\t\tfake_reviewers = mappings[(i,j)]\n",
    "\t\t\tfor user in fake_reviewers:\n",
    "\t\t\t\tfaker_id.append(user[0])\n",
    "\t\t\t\tfaker_list.append(mapping[user[0]])\n",
    "                \n",
    "print('No. of fake reviewers: ', len(faker_list))\n",
    "print('list of fake user IDs is\\n', faker_list)\n",
    "\n",
    "# Visualizing the results (SOM in a 2-D plot)\n",
    "from pylab import bone, pcolor, colorbar, plot, show\n",
    "bone() # creates a white window\n",
    "pcolor(som.distance_map().T)\n",
    "colorbar()\n",
    "markers = ['o', 's']\n",
    "colors = ['r', 'g']\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Unsupervised learning of SOM makes use of unlabelled data by trying to achieve various goals. One may look for hidden patterns, try to cluster similar data points together or even seek outliers in a dataset. In our case, we grouping and extracting feature of suspicious group, so instead of detacting outliers, we try to cluster some group toghther. Orriginally in the high dimention we need more calculation and space, now our data is more visualizable. After getting the cluster, we will recalculate all the indicators so as to determine which groups are fake. In the paper they use NDCG to justify their result, but we don't have the lable data. Therefore, we will meature the accuracy of top K cluster by comparing all of our result to see the intersaction.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Result Evaluation & Analysis\n",
    "\n",
    "The output of the SOM gives the reviewer IDs that are fake.\n",
    "\n",
    "Initially, by looking at the dataset we had assumed that a few reveiwers were definitely fake reviwers based on the content of their reviews. But, surprisingly the results we obtained were pretty different. Those reviewer IDs were not returned by the SOM as fake ones. This might be due to the fact that it is very difficult to distinguish fake reviewers from normal ones by just observing the content of their reviews.\n",
    "\n",
    "Grouping reviewers using the criteria that we chose helped us identify reviewer patterns which helped us understand the beahavior of fake reviewers better. This also helped us catch fake reviewers who were difficult to identify by from just the content of their reviews. \n",
    "\n",
    "The SOM at times gives different output every time it is run. This is because the weights of the neurons are initialized randomly every time it is run. This is an issue with SOM and the variance can be minimized by letting the SOM run for a large number of iterations. Typically, the number of iterations should be 20 times the size of the dataset. In our dataset the number of groups is about 5000. Hence, we have run the SOM for 100,000 iterations and do it 100 times so as to get the average distribution of reality.\n",
    "\n",
    "Overall, SOM does find fake reviewers but it is pretty unpredictable. We chose SOM as our clustering algorithm because it is great at detecting frauds and is used in detecting credit card frauds. The problem we had was pretty similar but in this case we had to put reviewers into groups and design their feature vectors. Although we had hoped that SOM would work very well to solve our problem but it seems that it requires a lot of dataset, iterations and parameter tuning to detect fake reviewers and reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You may use table as html like this\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <td> row head 1 \n",
    "            <td> row head 2\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td> row body 1\n",
    "            <td> row body 2\n",
    "        </tr>   \n",
    "    </tbody>    \n",
    "</table>\n",
    "\n",
    "## You may insert photo like this\n",
    "\n",
    "<img src=\"./images/test.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://liu.cs.uic.edu/download/data/\n",
    "\n",
    "[2] https://www.cs.uic.edu/~liub/publications/WWW-2012-group-spam-camera-final.pdf\n",
    "\n",
    "[3] https://www.hindawi.com/journals/mpe/2016/4935792/\n",
    "\n",
    "[4] https://www.cs.uic.edu/~liub/FBS/fake-reviews.html\n",
    "\n",
    "[5] http://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "[6] http://www.bbc.com/news/technology-22166606\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {delete before submission} Proposal -- You may need to refer to this paragraph\n",
    "\n",
    "Prior works on opinion spam focused on detecting fake reviews and individual fake reviewers. However, a fake reviewer group (a group of reviewers who work collaboratively to write fake reviews) is even more damaging as they can take total control of the sentiment on the target product due to its size. Therefore, we are going to follow our base paper trying to identify fake reviews by clustering fake spammers into groups, which are also called group spammers[1]. Group spamming refers to a group of reviewers writing fake reviews together to promote or to demote some target products. The base paper[1] we choose has done experiments that show it is hard to detect spammer groups using review content features or even indicators for detecting abnormal behaviors of individual spammers because a group has more manpower to post reviews and thus, each member may no longer appear to behave abnormally. A group of reviewers refers to a set of reviewer-ids. The actual reviewers behind the ids could be a single person with multiple ids (sockpuppet), multiple persons, or a combination of both.\n",
    "\n",
    "\n",
    "Therefore, we will also implement a relation-based model “GSRank” described in [2] using an Artificial Neural Network [5] as the following figure1. The GS rank algorithm is an unsupervised iterative algorithm that works differently from the traditional supervised learning approach to spam detection. The paper [2] has concluded after experiments that “GSRank” performs better than the state-of-the-art supervised classification, regression, and learning to rank algorithms. Basically, we also follow this paper [2] to build a more effective model which can consider the inter-relationship among products, groups, and group members in computing group spamicity. In other words, we will try to reimplement the paper from scratch. In conclusion, after getting two different result as we mentioned above, we will evaluate with Precision, Recall and NDCG.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {delete before submission}  Requirement\n",
    "\n",
    "\n",
    "\n",
    "Your main deliverable is your project notebook. This will act as both a written report plus a walkthrough of your code. This is the critical piece that will document and detail your project experience. We expect your project notebook to tell us the story of your project -- from initial question and data collection, to initial exploratory data analysis, perhaps to a revised question, to analyses, visualizations, and key takeaways.\n",
    "\n",
    "\n",
    "Note that we do not want to see a completely raw, moment-by-moment accounting of your project (include all 99 missteps and dead-ends); rather, you should carefully put together your final project notebook for submission that captures the key steps along the way.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cs670]",
   "language": "python",
   "name": "conda-env-cs670-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
