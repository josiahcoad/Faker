{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CSCE 670 :: Information Storage and Retrieval - Final Project Report\n",
    "\n",
    "<h1><center>Amazon Fake Reviews Classifier and Analysis</center></h1>\n",
    "<h4><center> Josiah Coad, Savinay Narendra, Sheelabhadra Dey, Chaiwei Chang, Kevin Chang</center></h4>\n",
    "Github: [Click Me](https://github.com/josiahcoad/Faker)<br>\n",
    "Data: Please refer to library folder in the repository. (Source: Amazon)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  It has become a common practice for online reviews to have a major impact on the decision of the potential customers of that product. Positive reviews can result in significant financial gains. This gives a strong incentive for fraud reviews, also commonly called opinion scamming. Opinion scamming includes fake blogs, reviews, deceptive advertising and more. Our research is accordingly focusing on product reviews based on the Amazon dataset both from University of Illinois at Chicago[1] and Professor Caverlee’s lab at Texas A&M University. Reports indicated 2-6% reviews on average are fake with up to 20% on sites such as Yelp. This leads to an unrealistic representation of places and products on the internet. Additionally, there are some fake review cases in the news for example [6].\n",
    "\n",
    "  Based on existing machine learnign techniques, we applied unsupervised clustering and observe that whether the spam/fake reviewers can be clustered together. In order to enhance the performance of learning process, we preprocessed the data to focus on suspicious group or individual because we believe that the some obivious features are similar within these groups.\n",
    "  \n",
    "  The key difficulty in determining fake reviews is that it is extremely hard for humans to identify fake reviews. In one related work, it was said to take a team of industry experts eight weeks to develop a labeled data set. We believe that a machine can do better at identifying the fake reviews by extracting the implicit information or behavior feature inside the reviews efficiently.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Initialization\n",
    "\n",
    "To initialize tha data before classifying, we categorized them into groups of members because people who write fake reviews will appear some similar behavior so that this can be a clue that we find them out through group. Afterwards, we explore eight different features on each group and utilize them in the vector space. Now that we have the vector of each group, we implement scaling and normalization to find out the top k, scores of being fake reviewer. Eventually, we feed our input space to Self-Oorganising map(SOM) machine and further observe the result of clustering, and compare the results with  different approaches such as word2vec and basic cosine similarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Group Indicator\n",
    "\n",
    "By grouping our reviewers together in hand tune features of rule-base design, hopeing to get a better result of detecting fake review. Here we defined the rules of grouping: (1) If a member rates a certain product with ONE or FIVE stars, he/she is eligible for group. (2) If eligible members commonly rate on the same three or more products, we will form them a group. Note that one member can be in different groups. For example, member A, B, C, and D rate on the same three products i, j, and k as five stars. Therefore, they will form a group. Furthermore, member A, B and C rate products i, j, k, and l as five stars, they will form another group. The condition of rules has the hueristic meaning because one that rate extreme scors may be attempting to make impact on certain product, and if all the members in a group commonly rate on certains product, they has higher probabilities to be conspiracy. By applying these rules, we obtain 1307 different groups and 82577 products that has been rated over once. The related codework is shown on two cells below.\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 99117\n",
      "Number of reviewers: 4743\n",
      "Number of reviewers with 3+ reviews rated 1 or 5 star: 3268\n",
      "('A2R1SS382YW679', [])\n",
      "\n",
      "('A135L0KYJC3K4H', [])\n",
      "\n",
      "('A1NGEEN1F7FVMK', [])\n",
      "\n",
      "('A3L7Z3ZXGIMWD3', [])\n",
      "Number of groups:  261\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "\n",
    "def parserJSON(path, numLines=None):\n",
    "  numLines = numLines or len(open(path).read().split(\"\\n\")) - 1\n",
    "  with open(path) as txt:\n",
    "    reviews = [eval(next(txt)) for x in range(numLines)]\n",
    "  print(\"Number of reviews:\", len(reviews))\n",
    "  return reviews\n",
    "\n",
    "\n",
    "def get_reviewers(reviews):\n",
    "   reviewers = {}\n",
    "   for review in reviews:\n",
    "      reviewerId = review[\"memberId\"]\n",
    "      if reviewerId not in reviewers:\n",
    "         reviewers[reviewerId] = [review]\n",
    "      else:\n",
    "         reviewers[reviewerId].append(review)\n",
    "   print(\"Number of reviewers:\", len(reviewers))\n",
    "   return reviewers\n",
    "\n",
    "def remove_lessthan3(reviewers_reviews):\n",
    "   final = {}\n",
    "   for reviewer, reviews in reviewers_reviews.items():\n",
    "      reviews = list(filter(lambda review: review[\"Rate\"] == 1 or review[\"Rate\"] == 5, reviews))\n",
    "      if len(reviews) >= 3:\n",
    "            final[reviewer] = sorted(reviews, key=lambda review: review[\"productId\"])\n",
    "   print(\"Number of reviewers with 3+ reviews rated 1 or 5 star:\", len(final))\n",
    "   return final\n",
    "\n",
    "def get_products(reviews):\n",
    "   products = {}\n",
    "   for review in reviews:\n",
    "      productId = review[\"productId\"]\n",
    "      if productId not in products:\n",
    "         products[productId] = [review]\n",
    "      else:\n",
    "         products[productId].append(review)\n",
    "   return products\n",
    "\n",
    "def normalizedVector(vector):\n",
    "    total = 0\n",
    "    for key in vector:\n",
    "        total += vector[key] ** 2\n",
    "    total = total ** 0.5\n",
    "    for key in vector:\n",
    "        vector[key] /= total\n",
    "    return vector\n",
    "\n",
    "\n",
    "# from modules.amazon_parser import *\n",
    "reviewers_products = []\n",
    "\n",
    "# get a list of dictionary items which represent each review object (including metadata like product id and user id) \n",
    "reviews = parserJSON('./library/amazon-review-data.json')\n",
    "# get a list of tuples with user as first entry and a list of the review objects their part of as the second\n",
    "reviewers_reviews_dict = get_reviewers(reviews)\n",
    "reviewers_reviews = reviewers_reviews_dict\n",
    "# remove all reviewers who reviewed less than 3 products with ratings other than 1 or 5\n",
    "reviewers_reviews = remove_lessthan3(reviewers_reviews)\n",
    "\n",
    "# create a new list of tuples... with first entry being the reviewer \n",
    "# and second being a list of the product ids reviewed\n",
    "for reviewer, reviews in reviewers_reviews.items():\n",
    "   reviewers_products.append( (reviewer, [review[\"productId\"] for review in reviews]) )\n",
    "\n",
    "# get a sorted list of reviews that a user left for products which match 'productIds'\n",
    "def get_product_reviews(productIds, userId):\n",
    "   return [review for review in reviewers_reviews_dict[userId] if review[\"productId\"] in productIds]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The eight spam features we applied are describes as below [2]:\n",
    "\n",
    "\n",
    "<ol>\n",
    "    <li>Group Time Window (GTW):</li>\n",
    "    <p>The assumption is that the group tends to work together within a period of time, which means we will give higher scores to those rate the same product within a short time period. In our experiment, we set up the window as four days.</p>\n",
    "    \n",
    "\\begin{equation*}\n",
    "    GTW(g, p)=\n",
    "    \\begin{cases}\n",
    "      0, & \\text{if}\\ L(g, p) - F(g, p)>\\tau \\\\\n",
    "      1-\\frac{L(g, p) - F(g, p)}{\\tau}, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "    <li>Group Deviation (GD):</li>\n",
    "    <p>If one group has higher deviation with other members' rating on the certain product, they all become suspicious so that we will give them higher score in this indicator</p>\n",
    "$$D(g, p) = \\frac{r_{p,g} - \\bar{r}_{p,g}}{4}$$\n",
    "\n",
    "    <li>Group Content Similarity (GCS):</li>\n",
    "    <p>This indicatoe capture the similarity of the review text within the group member. Because the fake review may duplicate the review from their member or make a very smilar one.</p>\n",
    "$$CS_G(g,p)=avg_{m_i,m_j\\in g,i < j} \\Big( cosine(c(m_i,p),c(m_j,p) \\Big)$$\n",
    "\n",
    "    <li>Group Member Content Similarity (GMCS):</li>\n",
    "    <p>This feature is similar to the previous one, while this one focuses on the member to similar products. Due to the taxing work of writing a new review, spammer tend to modified the existing fake review and paste on the similar products.</p>\n",
    "$$CS_M(g,m)=avg_{p_i,p_j\\in P_g,i < j} \\Big( cosine(c(m,p_i),c(m,p_j) \\Big)$$\n",
    "\n",
    "\n",
    "    <li>Group Early Time Frame (GETF):</li>\n",
    "    <p>This indicator features the importance of rating time on products, i.e. if one rates a product at the first beginging, it is more suspicious to be fake because it seems to make impact on a product when it has a few rating.</p>\n",
    "\\begin{equation*}\n",
    "    GTF(g, p)=\n",
    "    \\begin{cases}\n",
    "      0, & \\text{if}\\ L(g, p) - A(p)>\\beta \\\\\n",
    "      1-\\frac{L(g, p) - A(p)}{\\beta}, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "    <li>Group Size Ratio (GSR):</li>\n",
    "    <p>This features the ratio of the number of members in a group to the total number of reviews for a certain product. Gigher group ratio means the group dominates the rating and is more likely to be spammer.</p>\n",
    "$$GSR_p(g,p)= \\frac{|g|}{|M_p|}$$\n",
    "\n",
    "    <li>Group Size (GS):</li>\n",
    "    <p>For larger groups, members is less likely to be together by chance so that this is a simple intuitive feature.</p>\n",
    "$$GS(g)= \\frac{|g|}{max(|P_{gi}|)}$$\n",
    "\n",
    "    <li>Group Support Count (GSUP):</li>\n",
    "    <p>This indicator is the total number of products towards which the group has worked together. Group with higher support count will be given a higher score since they are likely to be spammer. Note that the score is normalized to [0,1].</p>\n",
    "$$GSUP(g)= \\frac{|P_g|}{max(|P_{gi}|)}$$\n",
    "\n",
    "Note that each value x attained by a feature $f(0 ≤ x ≤ 1)$ as $f \\in [0,1]$.\n",
    "    \n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A2R1SS382YW679', [])\n",
      "\n",
      "('A135L0KYJC3K4H', [])\n",
      "\n",
      "('A1NGEEN1F7FVMK', [])\n",
      "\n",
      "('A3L7Z3ZXGIMWD3', [])\n",
      "Number of groups:  261\n",
      "Number of reviews: 99117\n"
     ]
    }
   ],
   "source": [
    "groups = []\n",
    "for i in range(len(reviewers_products)-1):\n",
    "   ref_user = reviewers_products[i]\n",
    "   newgroup = [ref_user]\n",
    "   for j in range(i+1, len(reviewers_products)):\n",
    "      compare_user = reviewers_products[j]\n",
    "      shared_products = set(ref_user[1]).intersection(set(compare_user[1]))\n",
    "      if len(shared_products) >= 3:\n",
    "         newgroup.append(compare_user)\n",
    "   if len(newgroup) >= 2:\n",
    "      group_products = sorted(list(set(ref_user[1]).intersection(*[set(user[1]) for user in newgroup])))\n",
    "      newgroup = [( user[0], get_product_reviews(group_products, user[0]) ) for user in newgroup]\n",
    "      groups.append(newgroup)\n",
    "print(*groups[0], sep=\"\\n\\n\")\n",
    "print(\"Number of groups: \", len(groups))\n",
    "\n",
    "import math, re, string\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def purify(s):\n",
    "   s = s.translate(None, string.punctuation)\n",
    "   s = re.sub('(\\s+)(a|an|and|but|the)(\\s+)', ' ', s)\n",
    "   s = [ps.stem(word.lower()) for word in re.split('\\W+', s)]\n",
    "   # print(s)\n",
    "   return s\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "def cosine_sim(string1, string2):\n",
    "   count1 = defaultdict(int)\n",
    "   count2 = defaultdict(int)\n",
    "   for word in purify(string1):\n",
    "      count1[ps.stem(word.lower())] += 1\n",
    "   for word in purify(string2):\n",
    "      count2[ps.stem(word.lower())] += 1\n",
    "   dot_product = sum(count1.get(key, 0)*count2.get(key, 0) for key in count1)\n",
    "   magnitude = math.sqrt(sum([int(val)**2 for val in count1.values()])) * math.sqrt(sum([int(val)**2 for val in count2.values()]))\n",
    "   return dot_product/magnitude if magnitude else 0\n",
    "\n",
    "from numpy import mean as avg\n",
    "\n",
    "review_objects = parserJSON('./library/amazon-review-data.json')\n",
    "\n",
    "products_dict  = get_products(review_objects) # create a dict with product ID as the key and a list of the product's reviews as the value\n",
    "\n",
    "\n",
    "products_dict = get_products(review_objects) # create a dict with product ID as the key and a list of the product's reviews as the value\n",
    "\n",
    "\n",
    "MAX_USERS = 5 # found previously\n",
    "MAX_PRODS = 7 # found previously\n",
    "\n",
    "\n",
    "with open(\"./library/groups.txt\") as f:\n",
    "   groups = eval(f.read())\n",
    "\n",
    "# takes a dictionary of groups which are organized by groupID as the key and a list of tuples as the value\n",
    "# return a list of groups where each group is structured as: [(product, [reviews]), (product, [reviews])]\n",
    "def organize_by_product(groups_dict):\n",
    "   group_list = []\n",
    "   for groupId, group in groups_dict.items():\n",
    "      reviews = []\n",
    "      for user, user_reviews in group:\n",
    "         reviews.extend(user_reviews)\n",
    "      products_reviews = defaultdict(list)\n",
    "      for review in reviews:\n",
    "         products_reviews[review[\"productId\"]].append(review)\n",
    "      group_list.append( products_reviews.items() )\n",
    "   return group_list\n",
    "\n",
    "groups_by_products = organize_by_product(groups)\n",
    "\n",
    "# takes a dictionary of groups which are organized by groupID as the key and a list of tuples as the value\n",
    "# return a list of groups where each group is structured as: [(reviewer, [reviews]), (reviewer, [reviews])]\n",
    "def organize_by_user(groups_dict):\n",
    "   return [groups_dict[key] for key in groups_dict]\n",
    "\n",
    "groups_by_reviewers = organize_by_user(groups)\n",
    "\n",
    "def get_avg(Name):\n",
    "    if(len(products_dict[Name])>0):\n",
    "        count = 0\n",
    "        sum = 0\n",
    "        for i in range(len(products_dict[Name])):\n",
    "            sum+= products_dict[Name][i][\"Rate\"]\n",
    "            count+=1\n",
    "        return float(sum/count)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Group Deviation (GD)\n",
    "def GD(group):\n",
    "    Deviation = []\n",
    "    handle = set()\n",
    "    for i in range(len(group)):\n",
    "        cur_user = group[i]\n",
    "        for item in cur_user[1]:\n",
    "            if(item[\"productId\"] not in handle):\n",
    "                handle.add(item[\"productId\"])\n",
    "                if(item[\"Rate\"]==5):\n",
    "                    Deviation.append(abs(5-get_avg(item[\"productId\"]))/4)\n",
    "                elif(cur_user[1][1][\"Rate\"]==1):\n",
    "                    Deviation.append(abs(get_avg(item[\"productId\"])-1)/4)\n",
    "    return max(Deviation)\n",
    "\n",
    "# Group Member Content Similarity\n",
    "def GMCS(group):\n",
    "  MCS = []\n",
    "  count = []\n",
    "  for i in range(len(group)):\n",
    "    cur_user = group[i]\n",
    "    MCS.append(0)\n",
    "    count.append(0)\n",
    "    for x in range(len(cur_user[1])-1):#each review\n",
    "      for y in range(x+1,len(cur_user[1])):\n",
    "        MCS[i]+=cosine_sim(cur_user[1][x][\"reviewText\"], cur_user[1][y][\"reviewText\"])    \n",
    "        count[i]+=1 \n",
    "    MCS[i]/=count[i]\n",
    "  Sum = 0\n",
    "  for indi in MCS:\n",
    "    Sum+=indi\n",
    "  return float(Sum)/len(group)\n",
    "\n",
    "# Group Size (GS) (number of users in group)\n",
    "def GS(group_by_users):\n",
    "    return float(len(group_by_users)) / MAX_USERS\n",
    "\n",
    "# Group Size Ratio (GSR) (returns 1 if each product in the group were only reviewed by the group members)\n",
    "def GSR(group_by_products):\n",
    "  return avg ( [gsr(product, reviews) for product, reviews in group_by_products] )\n",
    "\n",
    "def gsr(product, reviews):\n",
    "  return float(len(reviews)) / len(products_dict[product])\n",
    "# ------------------------\n",
    "\n",
    "def GTW(group):\n",
    "   return max([prod_TW(reviews) for product, reviews in group])\n",
    "\n",
    "def prod_TW(reviews):\n",
    "   GTW_MAXTIME = 345600 # number of seconds in 4 days\n",
    "   timestamps = [float(review[\"Date\"]) for review in reviews]\n",
    "   _range = max(timestamps)-min(timestamps)\n",
    "   return 1-_range/GTW_MAXTIME if _range < GTW_MAXTIME else 0\n",
    "\n",
    "def GCS(group):\n",
    "   return max([CS(reviews) for product, reviews in group])\n",
    "\n",
    "def CS(reviews):\n",
    "   texts = [review[\"reviewText\"] for review in reviews]\n",
    "   return avg([cosine_sim(review1, review2) for review1 in texts for review2 in texts])\n",
    "\n",
    "def GETF(group):\n",
    "   return max([GTF(product, reviews) for product, reviews in group])\n",
    "\n",
    "def GTF(product, reviews):\n",
    "   GTF_MAXTIME = 15552000 # seconds in 6 months\n",
    "   earliest_product_review = min([float(review[\"Date\"]) for review in products_dict[product]])\n",
    "   latest_group_review = max([float(review[\"Date\"]) for review in reviews])\n",
    "   _range = latest_group_review-earliest_product_review\n",
    "   return 1-_range/GTF_MAXTIME if _range < GTF_MAXTIME else 0\n",
    "\n",
    "# Group Support Count (GSUP) (number of products in group)\n",
    "def GSUP(group):\n",
    "  return float(len(group)) / MAX_PRODS\n",
    "\n",
    "# Sum Scores\n",
    "def scores(gbp, gbr):\n",
    "   return [GCS(gbp), GTW(gbp), GETF(gbp), GSUP(gbp), GS(gbr), GSR(gbp), GD(gbr), GMCS(gbr)]\n",
    "\n",
    "\n",
    "def get_all_scores():\n",
    "  all_scores = []\n",
    "  for i in range(len(groups_by_reviewers)):\n",
    "     all_scores.append(scores(groups_by_products[i], groups_by_reviewers[i]))\n",
    "  return all_scores\n",
    "\n",
    "# scores = [( i, sum(score) ) for i, score in enumerate(get_all_scores())]\n",
    "# fakest_indexes = sorted(scores, lambda k: k[1], reverse=True)\n",
    "# for top in fakest_indexes[:5]:\n",
    "#   fakest_users = [reviewer for reviewer, review in groups_by_reviewers[fakest_index]]\n",
    "#   print(fakest_users)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Individual Indicator\n",
    "\n",
    "Moreover, we should also find some of the features from individual. Although group behaviors are important, they hide a lot of details about its members. Clearly, individual members’ behaviors also give signals for group spamming. We get some indicators as following code, and append to our feature list to become latter traning input.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering\n",
    "### Self-Organizing Map\n",
    "A self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional, discretized representation of the input space of the training samples, called a map. In our case, after the exprirement, we decided to set our map into 2D. This dimensionality reduction makes SOMs useful for visualizing low-dimensional views of high-dimensional data. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning such as backpropagation with gradient descent, and in the sense that SOM use a neighborhood function to preserve the topological properties of the input space. \n",
    "### Training\n",
    "A self-organizing map consists of components called nodes or neurons. Associated with each node are a weight vector of the same dimension as the input data vectors, and a position in the map space. At first, we assign the weights of the neurons be initialized to small random value. Than, the neuron whose weight vector is most similar to the random sampling input is called the best matching unit(BMU). The weights of the BMU and neurons close to it in the SOM lattice are adjusted towards the input vector. During mapping, there will be one single winning neuron: the neuron whose weight vector lies closest to the input vector. This can be simply determined by calculating the Euclidean distance between input vector and weight vector. By the same time, we also up date the neighbors that fulfill neurons cooperation that near-by neurons on the lattice get a chance to adapt.\n",
    "Our updating weight function is $$\\vec{w}_j :: \\vec{w}_j + \\eta h(j,i(x))(\\vec{x}-\\vec{w}_j) $$ where $$i(x) = argmin_j|\\vec{x}-\\vec{w}_j|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "import math as math\n",
    "from random import randint\n",
    "class som:\n",
    "    def __init__(self, input,maxIterations=10,sigmaInitial = 4,somCol=3, somRow=3):\n",
    "\t\tself.somCol = somCol\n",
    "\t\tself.somRow = somRow\n",
    "                self.input  = input\n",
    "                self.maxIterations = maxIterations;\n",
    "                self.sigmaInitial = sigmaInitial\n",
    "    \n",
    " \n",
    "    def trainmodel(self):\n",
    "        input = self.input\n",
    "       \n",
    "        somCol = self.somCol\n",
    "        somRow = self.somRow\n",
    "        inputvectorlen = len(input[0,:])\n",
    "        inputsSize = len(input[:,0])\n",
    "        #initialise neurons layer.\n",
    "        somMap = numpy.zeros(shape=(somCol,somRow,inputvectorlen))\n",
    "        #print somMap\n",
    "\n",
    "        #Max number of iterations\n",
    "        maxIterations = self.maxIterations\n",
    "\n",
    "        # Initial effective width\n",
    "        sigmaInitial = self.sigmaInitial\n",
    "\n",
    "        # Time constant for sigma\n",
    "        t1 = maxIterations / numpy.log(sigmaInitial)\n",
    "\n",
    "        #Initialise matrix to store eucledian distances.\n",
    "        #euclideanD = numpy.zeros(shape =(somRow, somCol))\n",
    "       \n",
    "        # Initialize 10x10 matrix to store neighbourhood functions\n",
    "        # of each neurons on the map\n",
    "        neighbourhoodFunctionVal = numpy.zeros(shape =(somRow, somCol));\n",
    "\n",
    "        # initial learning rate\n",
    "        learningRateInitial = 0.1;\n",
    "\n",
    "        #time constant for eta\n",
    "        t2 = maxIterations;\n",
    "        #Assign random weight vectors for all the neurons\n",
    "        for num in range (0,(somRow)):\n",
    "            for iter in range (0,(somCol)):\n",
    "                #Squeezed the matrix into an ndarray.\n",
    "                somMap[num,iter,:] = numpy.squeeze(numpy.random.rand(inputvectorlen,1))\n",
    "                    \n",
    "#         print \"Again #printing the som with randomly initialised weight vectors\"\n",
    "#         print somMap\n",
    "\n",
    "        count = 1;\n",
    "        while(count < maxIterations):\n",
    "            \n",
    "            sigma = sigmaInitial * numpy.exp(-count/t1)\n",
    "            variance = pow(sigma,2) \t\n",
    "            eta = learningRateInitial * numpy.exp(-count/t2)\n",
    "    \n",
    "            #Prevent eta from falling below 0.01\n",
    "            if (eta < 0.01):\n",
    "                eta = 0.01\n",
    "            #Randomly select a weight vector from the input weight vectors.\n",
    "            inputIndex = randint(0,inputsSize-1)\n",
    "            selectedWeightVector = input[inputIndex,:]\n",
    "          \n",
    "            #Select the winning neuron which has the weight vector closest to that of selected input weight vector.\n",
    "            #Find the indices of minimum eucledian distance element.\n",
    "            mineuclideanD=numpy.linalg.norm(selectedWeightVector-somMap[0,0,:])\n",
    "            minr=0\n",
    "            minc=0\n",
    "            for num in range (0,somRow):\n",
    "                for iter in range (0,somCol):\n",
    "                    \n",
    "                    temp  = numpy.linalg.norm(selectedWeightVector-somMap[num,iter,:])\n",
    "                    \n",
    "                    if(temp <=mineuclideanD):\n",
    "                        minr=num\n",
    "                        minc=iter\n",
    "                        mineuclideanD=temp\n",
    "            #print euclideanD\n",
    "                   \n",
    "            #print 'indices are',minr,minc\n",
    "        \n",
    "            #compute the neighbourhood function for all the neurons\n",
    "            #For the winning noe\n",
    "            for r in range (0,somRow):\n",
    "                for c in range (0,somCol):\n",
    "                    if (r == minr & c == minc):  \n",
    "                        neighbourhoodFunctionVal[r, c] = 1;\n",
    "                        continue;\n",
    "                    else:\n",
    "                        distance = (minr - r)^2 + (minc - c)^2;\n",
    "                        neighbourhoodFunctionVal[r, c] = numpy.exp(-distance/(2*variance));\n",
    "            \n",
    "            #print 'neighbourhood functions are',neighbourhoodFunctionVal\n",
    "            #Update weights \n",
    "\n",
    "            for r in range (0,somRow):\n",
    "                for c in range (0,somCol):\n",
    "                    oldWeightVector = somMap[r, c,:]\n",
    "                    somMap[r, c,:]     = oldWeightVector + eta*neighbourhoodFunctionVal[r, c]*(selectedWeightVector - oldWeightVector)\n",
    "   \n",
    "            #Increment the counter\n",
    "            count +=1\n",
    "        \n",
    "        #Return updated map of neurons.\n",
    "        return somMap\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained model is [[[ 0.86873737  0.722772    0.37563978  0.60210187  0.83515825  0.73670491\n",
      "    0.11304375  0.36711602]\n",
      "  [ 0.86879263  0.80270152  0.69239661  0.89949096  0.75284733  0.71704691\n",
      "    0.75496031  0.77311934]\n",
      "  [ 0.87523371  0.52073934  0.70366966  0.20782134  0.42443584  0.43883043\n",
      "    0.46791777  0.73526197]\n",
      "  [ 0.39466976  0.55776331  0.61962557  0.7222089   0.56766668  0.14485464\n",
      "    0.36985992  0.59708854]\n",
      "  [ 0.36499381  0.60159268  0.68025203  0.26062214  0.61416041  0.36399045\n",
      "    0.07610384  0.51782515]\n",
      "  [ 0.74068678  0.64419985  0.85684241  0.82316189  0.38558285  0.2470232\n",
      "    0.35141184  0.30793878]\n",
      "  [ 0.81968896  0.55032513  0.9085003   0.49987986  0.63856564  0.21034179\n",
      "    0.2521369   0.58636993]\n",
      "  [ 0.69955258  0.93980972  0.98241344  0.84026044  0.39584135  0.31046762\n",
      "    0.07363662  0.38181184]\n",
      "  [ 0.760588    0.85069933  0.77104002  0.67463311  0.38024     0.23136249\n",
      "    0.13964104  0.2902743 ]\n",
      "  [ 0.66962262  0.8243615   0.95790111  0.74175401  0.43488037  0.46570014\n",
      "    0.25537785  0.38471626]]\n",
      "\n",
      " [[ 0.7077743   0.81929456  0.97022829  0.82055462  0.57625252  0.33604813\n",
      "    0.29558879  0.28574689]\n",
      "  [ 0.50788751  0.9226863   0.89521195  0.59538661  0.43827043  0.47330411\n",
      "    0.32931626  0.6041276 ]\n",
      "  [ 0.66181559  0.88110576  0.86482413  0.69032873  0.45998407  0.43563181\n",
      "    0.1564383   0.35349096]\n",
      "  [ 0.72281356  0.75840409  0.8095328   0.72925244  0.50104102  0.31465652\n",
      "    0.23377192  0.35618637]\n",
      "  [ 0.43367099  0.88896808  0.77343384  0.8584433   0.50605167  0.37889178\n",
      "    0.31546463  0.29480499]\n",
      "  [ 0.73734459  0.79311905  0.86095975  0.51783849  0.54200889  0.4966377\n",
      "    0.32401035  0.26965767]\n",
      "  [ 0.64178006  0.93128081  0.91859173  0.81254444  0.30974917  0.30086998\n",
      "    0.23894229  0.42618417]\n",
      "  [ 0.65972832  0.83778945  0.85101587  0.85752257  0.36172202  0.23916993\n",
      "    0.1069917   0.36258594]\n",
      "  [ 0.68045882  0.9464855   0.99432104  0.83061285  0.41024384  0.31962406\n",
      "    0.08783543  0.37289347]\n",
      "  [ 0.77981437  0.47797786  0.95741275  0.75320859  0.29124948  0.46594112\n",
      "    0.53069946  0.18876894]]\n",
      "\n",
      " [[ 0.54530985  0.64398698  0.85486647  0.60347019  0.61529144  0.62317043\n",
      "    0.30992203  0.44690576]\n",
      "  [ 0.78799268  0.60883613  0.85159546  0.84485794  0.33046971  0.20735818\n",
      "    0.18714116  0.30221005]\n",
      "  [ 0.75277463  0.85095695  0.90548516  0.62190967  0.48871045  0.46224656\n",
      "    0.12139656  0.29579785]\n",
      "  [ 0.78463381  0.94506281  0.85558539  0.72366645  0.34934925  0.36889898\n",
      "    0.11837204  0.41842648]\n",
      "  [ 0.82980199  0.4519756   0.51657086  0.44703401  0.57491366  0.54760591\n",
      "    0.56081415  0.64585327]\n",
      "  [ 0.69228995  0.92805108  0.94089997  0.83917967  0.40534735  0.31512766\n",
      "    0.08799655  0.37286555]\n",
      "  [ 0.7194996   0.7592595   0.92250134  0.69451361  0.39110148  0.40609004\n",
      "    0.16419571  0.33406923]\n",
      "  [ 0.76850138  0.78292435  0.87494098  0.66255057  0.49239529  0.34598473\n",
      "    0.29990254  0.46705268]\n",
      "  [ 0.64895827  0.59990381  0.94687142  0.73696127  0.55106981  0.27422549\n",
      "    0.19298003  0.31755396]\n",
      "  [ 0.77312681  0.81513463  0.92498622  0.50069787  0.25417303  0.36532855\n",
      "    0.39745691  0.63324731]]\n",
      "\n",
      " [[ 0.65302485  0.86701667  0.85672071  0.7620356   0.44345667  0.37794594\n",
      "    0.20813933  0.35368075]\n",
      "  [ 0.7505606   0.76658657  0.83216137  0.67867546  0.50670945  0.31349121\n",
      "    0.15041457  0.55896858]\n",
      "  [ 0.80171616  0.78417365  0.77861648  0.68971875  0.4093956   0.29692501\n",
      "    0.38592369  0.55171796]\n",
      "  [ 0.83596627  0.67481479  0.69094948  0.78621779  0.25007465  0.33367403\n",
      "    0.31933107  0.58272497]\n",
      "  [ 0.5287315   0.70584264  0.61788058  0.51219357  0.24552512  0.36156654\n",
      "    0.07319912  0.37535984]\n",
      "  [ 0.63848434  0.91400928  0.96329651  0.70796901  0.35672902  0.31091043\n",
      "    0.2405399   0.32496602]\n",
      "  [ 0.65671802  0.97242393  0.98873069  0.85182421  0.43084701  0.3148642\n",
      "    0.07494899  0.3734023 ]\n",
      "  [ 0.60836395  0.52215945  0.55513178  0.59012799  0.31375118  0.26065041\n",
      "    0.05202302  0.60301067]\n",
      "  [ 0.78500074  0.72663006  0.86763074  0.68090597  0.30281866  0.19140928\n",
      "    0.17450501  0.42304247]\n",
      "  [ 0.6010185   0.81849356  0.44929468  0.35069686  0.29287322  0.53620404\n",
      "    0.40072648  0.3167293 ]]\n",
      "\n",
      " [[ 0.67401889  0.91875194  0.75983947  0.64064982  0.35327657  0.36876439\n",
      "    0.31664436  0.55564938]\n",
      "  [ 0.77823142  0.94503668  0.9492159   0.67061399  0.47536747  0.36483018\n",
      "    0.18058429  0.37741202]\n",
      "  [ 0.66341273  0.54995006  0.95361301  0.61582935  0.2811858   0.25938373\n",
      "    0.08633258  0.33990946]\n",
      "  [ 0.64581487  0.69222173  0.88824901  0.67966496  0.26685171  0.26838268\n",
      "    0.21609266  0.53252843]\n",
      "  [ 0.83853691  0.58884832  0.66743077  0.77249371  0.32465107  0.19920122\n",
      "    0.47540831  0.38318655]\n",
      "  [ 0.47515259  0.59454769  0.94507818  0.56184032  0.35266256  0.44810606\n",
      "    0.05301123  0.44018603]\n",
      "  [ 0.76121809  0.71289887  0.91804849  0.81427651  0.61628922  0.52055843\n",
      "    0.49292547  0.26641648]\n",
      "  [ 0.79219883  0.9032216   0.93950934  0.50201054  0.35852646  0.29082658\n",
      "    0.15521489  0.36462306]\n",
      "  [ 0.67480845  0.68883212  0.95366025  0.80801065  0.4578997   0.45884866\n",
      "    0.19397343  0.39827966]\n",
      "  [ 0.78891318  0.7807259   0.71413145  0.69443633  0.47520474  0.44497261\n",
      "    0.06998048  0.55266588]]\n",
      "\n",
      " [[ 0.73649287  1.03630129  1.09113945  0.81777607  0.36730403  0.3527157\n",
      "    0.00471259  0.36724086]\n",
      "  [ 0.75626604  0.94469354  0.97745364  0.72793732  0.40764001  0.40532729\n",
      "    0.06007987  0.40899315]\n",
      "  [ 0.85667148  1.36249898  1.20412985  0.62351863  0.20999009  0.40268181\n",
      "   -0.31765463  0.2431439 ]\n",
      "  [ 0.82083281  1.22623078  1.25266442  0.7116025   0.27286041  0.4436594\n",
      "    0.00643763  0.28597164]\n",
      "  [ 0.8097182   0.54231414  0.45122985  0.45793942  0.22600877  0.56943648\n",
      "    0.24706718  0.58064977]\n",
      "  [ 0.39508397  0.7872708   0.38470664  0.42730537  0.3684913   0.55995582\n",
      "    0.36122803  0.30330988]\n",
      "  [ 0.74196975  0.59251736  0.89132731  0.80689076  0.49547609  0.42822263\n",
      "    0.47653475  0.26082662]\n",
      "  [ 0.81449211  0.51351753  0.7926131   0.81567642  0.50543772  0.51895167\n",
      "    0.30315485  0.36319831]\n",
      "  [ 0.79394772  0.87197812  0.83774648  0.58046642  0.57860037  0.21073223\n",
      "    0.09843974  0.51118369]\n",
      "  [ 0.56242202  0.58415067  0.63382783  0.66224558  0.48602891  0.45024497\n",
      "    0.1099847   0.39598649]]\n",
      "\n",
      " [[ 0.73734196  0.93020733  0.98961584  0.76889507  0.4017811   0.347543\n",
      "    0.08652387  0.38714093]\n",
      "  [ 0.77598878  1.00826455  1.04975686  0.86717181  0.40566137  0.27251539\n",
      "    0.09695686  0.30578286]\n",
      "  [ 0.84496783  0.98951782  1.09130132  0.91329501  0.41258776  0.38171975\n",
      "   -0.0193586   0.39526953]\n",
      "  [ 1.07130201  1.27564992  1.2510928   0.91631353  0.34952239  0.61784402\n",
      "   -0.34426451  0.27742626]\n",
      "  [ 0.35355845  0.86682436  0.36843277  0.8222606   0.2869402   0.33587242\n",
      "    0.48357753  0.70718049]\n",
      "  [ 0.62848777  0.79090691  0.84063738  0.82391101  0.71169459  0.31837171\n",
      "    0.16728967  0.35870711]\n",
      "  [ 0.68477608  0.62856136  0.64536854  0.75470937  0.42620218  0.36782209\n",
      "    0.12272861  0.4379852 ]\n",
      "  [ 0.81197454  0.85213395  0.95704529  0.80242611  0.24918077  0.1297259\n",
      "    0.47645315  0.57171959]\n",
      "  [ 0.39826109  0.4391768   0.58868251  0.87863398  0.35041634  0.14725299\n",
      "    0.2627163   0.40285434]\n",
      "  [ 0.6704256   0.95506898  0.95050778  0.85887569  0.41358948  0.32024286\n",
      "    0.08293179  0.37003046]]\n",
      "\n",
      " [[ 0.7974139   1.16603558  1.09642792  0.70689459  0.24420075  0.42166077\n",
      "    0.07955835  0.34546805]\n",
      "  [ 0.86867086  1.08423559  1.14710194  0.78389516  0.31743987  0.21139944\n",
      "    0.04209478  0.31280337]\n",
      "  [ 0.71996908  0.98806975  1.0334479   0.8494425   0.41284425  0.22873591\n",
      "    0.10976924  0.35130364]\n",
      "  [ 0.74566119  0.93289471  0.98048342  0.71046805  0.39314082  0.40747177\n",
      "    0.05055279  0.41749265]\n",
      "  [ 0.51303728  0.54093678  0.66368937  0.49652898  0.66992606  0.61977112\n",
      "    0.3429718   0.61490109]\n",
      "  [ 0.75659938  0.40922837  0.939898    0.42112655  0.28500415  0.33902114\n",
      "    0.31396263  0.40773593]\n",
      "  [ 0.65147308  0.68920584  0.73563762  0.62399825  0.50785496  0.31319945\n",
      "    0.13808707  0.68173472]\n",
      "  [ 0.76541549  0.54715733  0.85098454  0.69517751  0.5885861   0.24766046\n",
      "    0.42434185  0.68779146]\n",
      "  [ 0.79070229  0.68398169  0.65382049  0.75962037  0.62077115  0.19393086\n",
      "    0.40443861  0.5239399 ]\n",
      "  [ 0.71306335  0.8472678   0.92664685  0.76698976  0.41860674  0.44171714\n",
      "    0.08450059  0.35472288]]\n",
      "\n",
      " [[ 0.75465832  1.02854363  1.02089378  0.84722351  0.35929796  0.28489437\n",
      "    0.01693085  0.36631963]\n",
      "  [ 0.90087166  1.01160234  1.17515562  0.7855719   0.46997667  0.33812081\n",
      "   -0.20007602  0.40184674]\n",
      "  [ 0.74404785  0.94614551  0.98382203  0.76182036  0.4099904   0.35063732\n",
      "    0.07988446  0.39111884]\n",
      "  [ 0.74076505  1.04159643  1.10103031  0.72460625  0.40201881  0.38733227\n",
      "    0.02459447  0.45029769]\n",
      "  [ 0.47184366  0.38421668  0.48046746  0.31249483  0.50489423  0.4331\n",
      "    0.35939628  0.55766153]\n",
      "  [ 0.81499138  0.61395161  0.83196909  0.59212734  0.55680791  0.44466438\n",
      "    0.3706036   0.71691688]\n",
      "  [ 0.69608161  0.4184827   0.43194023  0.37279038  0.60122731  0.47503036\n",
      "    0.05418232  0.31140647]\n",
      "  [ 0.32445183  0.39200873  0.47414145  0.82062478  0.74471831  0.57655986\n",
      "    0.46684618  0.5044992 ]\n",
      "  [ 0.795211    0.41620634  0.92253529  0.68597568  0.34313898  0.68200125\n",
      "    0.36405565  0.28021542]\n",
      "  [ 0.77625992  0.43974933  0.48399311  0.67672202  0.32863997  0.3914588\n",
      "    0.09720603  0.25536628]]\n",
      "\n",
      " [[ 0.66317574  0.95670227  0.9693792   0.84631701  0.40567739  0.31264161\n",
      "    0.07174081  0.36924906]\n",
      "  [ 0.46883889  0.57200281  0.73464645  0.48022449  0.46485591  0.44428657\n",
      "    0.43306523  0.46486585]\n",
      "  [ 0.81584668  0.62189205  0.62404831  0.69525414  0.61837467  0.16919869\n",
      "    0.27286387  0.3163621 ]\n",
      "  [ 0.46797907  0.74666079  0.51764216  0.64042471  0.25880183  0.5451821\n",
      "    0.34378264  0.36326597]\n",
      "  [ 0.80242384  1.01606173  1.03808088  0.72507427  0.38682715  0.43015375\n",
      "    0.041841    0.37798802]\n",
      "  [ 0.73844039  0.93329848  0.98506673  0.71190912  0.41518816  0.404866\n",
      "    0.0683943   0.42423182]\n",
      "  [ 0.78062286  1.25846455  1.3539448   0.72953759  0.15697162  0.52972119\n",
      "   -0.09589682  0.28271479]\n",
      "  [ 0.84109577  1.19419455  1.27051221  0.64820709  0.44935346  0.28339788\n",
      "   -0.00188089  0.30571896]\n",
      "  [ 0.82021158  0.74486352  0.72430679  0.48215597  0.54843376  0.28110671\n",
      "    0.38373693  0.43147194]\n",
      "  [ 0.47623828  0.5812287   0.5674994   0.72642432  0.23408403  0.47754694\n",
      "    0.28745091  0.46593204]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "with open(\"./library/groups.txt\") as f:\n",
    "    groups = eval(f.read())\n",
    "\n",
    "\n",
    "final_input = np.array(get_all_scores())\n",
    "somCol = 10\n",
    "somRow = 10\n",
    "som = som(final_input,12,4,somCol,somRow)\n",
    "ans =som.trainmodel()\n",
    "print ('trained model is',ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Spam Detection using Reviewer Centric Features\n",
    "\n",
    "Identifying spammers can improve detection of fake reviews, since many spammers share profile characteristics and activity patterns. We have studied various combinations of features engineered from reviewer profile characteristics and behavioral patterns. \n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def parserJSON(path, numLines=None):\n",
    "    numLines = numLines or len(open(path).read().split(\"\\n\")) - 1\n",
    "    with open(path) as txt:\n",
    "        reviews = [eval(next(txt)) for x in range(numLines)]\n",
    "    return reviews\n",
    "\n",
    "reviews = parserJSON('./library/amazon-review-data.json')\n",
    "\n",
    "reviewer_collection = defaultdict(lambda: defaultdict(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Maximum number of reviews\n",
    "\n",
    "It was observed that about 75 % of spammers write more than 5 reviews on any given day. Therefore, taking into account the number of reviews a user writes per day can help detect spammers since 90 % of legitimate reviewers never create more than one review on any given day. \n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maximum_reviews(reviews):\n",
    "    reviews_per_day = defaultdict(lambda: defaultdict(float))\n",
    "    avg_reviews_per_day = defaultdict(lambda: defaultdict(float))\n",
    "    for i in range(len(reviews)):\n",
    "        day = time.strftime('%Y%m%d', time.localtime(float(reviews[i][\"Date\"])))\n",
    "        reviews_per_day[reviews[i][\"memberId\"]][day] += 1\n",
    "\n",
    "    for key1 in reviews_per_day:\n",
    "        for key2 in reviews_per_day[key1]:\n",
    "            avg_reviews_per_day[key1] =  sum(reviews_per_day[key1].values())/len(reviews_per_day[key1].keys())\n",
    "    count = 0\n",
    "    for key in avg_reviews_per_day:\n",
    "        if avg_reviews_per_day[key] > 3:\n",
    "            count += 1\n",
    "    return avg_reviews_per_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Percentage of positive reviews\n",
    "\n",
    "Approximately 85 % of spammers wrote more than 80 % of their reviews as positive reviews, thus a high percentage of positive reviews might be an indication of an un- trustworthy reviewer. \n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_reviews(reviews):\n",
    "    sentiment_reviews = defaultdict(lambda: defaultdict(float))\n",
    "    for i in range(len(reviews)):\n",
    "        sentiment_reviews[reviews[i][\"memberId\"]][reviews[i][\"reviewId\"]] = reviews[i][\"Rate\"]\n",
    "    positive_reviews = defaultdict(lambda: float)\n",
    "\n",
    "    for key1 in sentiment_reviews:\n",
    "        count_pos = 0\n",
    "        for key2 in sentiment_reviews[key1]:\n",
    "            if sentiment_reviews[key1][key2] > 3:\n",
    "                count_pos += 1\n",
    "        positive_reviews[key1] = float(count_pos)/float(len(sentiment_reviews[key1].keys()))\n",
    "    return positive_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Review Length\n",
    "\n",
    "The average review length may be an important indication of reviewers with question- able intentions since about 80 % of spammers have no reviews longer than 135 words while more than 92 % of reliable reviewers have an average review length of greater than 200 words. \n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_length(reviews):\n",
    "    review_length_collection = defaultdict(lambda: defaultdict(float))\n",
    "    for i in range(len(reviews)):\n",
    "        review_length_collection[reviews[i][\"memberId\"]][reviews[i][\"reviewId\"]] = len(reviews[i][\"reviewText\"])\n",
    "    avg_review_length_collection = defaultdict(lambda: float)\n",
    "    for key1 in review_length_collection:\n",
    "        for key2 in review_length_collection[key1]:\n",
    "            avg_review_length_collection[key1] = float(sum(review_length_collection[key1].values()))/float(len(review_length_collection[key1].keys()))\n",
    "    return avg_review_length_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Reviewer deviation\n",
    "\n",
    "It was observed that spammers’ ratings tend to deviate from the average review rating at a far higher rate than legitimate reviewers, thus identifying user rating deviations may help in detection of dishonest reviewers. \n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reviewer_deviation(reviews):\n",
    "    product_rating = defaultdict(lambda: defaultdict(float))\n",
    "    for i in range(len(reviews)):\n",
    "        product_rating[reviews[i][\"productId\"]][\"memberId\"] = reviews[i][\"Rate\"]\n",
    "\n",
    "    avg_product_rating = defaultdict(lambda: float)\n",
    "\n",
    "    for key1 in product_rating:\n",
    "        for key2 in product_rating[key1]:\n",
    "            avg_product_rating[key1] = float(sum(product_rating[key1].values()))/float(len(product_rating[key1].keys()))\n",
    "\n",
    "    deviation_member_rating = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    avg_deviation_member_rating = defaultdict(lambda: float)\n",
    "\n",
    "    for i in range(len(reviews)):\n",
    "        deviation_member_rating[reviews[i][\"memberId\"]][reviews[i][\"productId\"]] = reviews[i][\"Rate\"] - avg_product_rating[reviews[i][\"productId\"]]\n",
    "\n",
    "    for key1 in deviation_member_rating:\n",
    "        for key2 in deviation_member_rating[key1]:\n",
    "            avg_deviation_member_rating[key1] = float(sum(deviation_member_rating[key1].values()))/float(len(deviation_member_rating[key1].keys()))\n",
    "\n",
    "    return avg_deviation_member_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Building the Training Data\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_feature_vector():\n",
    "    avg_reviews_per_day = maximum_reviews(reviews)\n",
    "    positive_reviews = pos_reviews(reviews)\n",
    "    avg_review_length_collection = review_length(reviews)\n",
    "    avg_deviation_member_rating = reviewer_deviation(reviews)\n",
    "\n",
    "    training_data = {} \n",
    "    count = 0\n",
    "    mapping_training_id_user_id = {}\n",
    "    for key in avg_reviews_per_day:\n",
    "        if key not in training_data:\n",
    "            training_data[key] = [count]\n",
    "            mapping_training_id_user_id[count] = key\n",
    "            count += 1\n",
    "        if avg_reviews_per_day[key] > 3:\n",
    "            training_data[key].append(100 * avg_reviews_per_day[key])\n",
    "        else:\n",
    "            training_data[key].append(0.1 * avg_reviews_per_day[key])\n",
    "\n",
    "    for key in positive_reviews:\n",
    "        if positive_reviews[key] > 0.8:\n",
    "            training_data[key].append(3 * positive_reviews[key])\n",
    "        else:\n",
    "            training_data[key].append(0.2 * positive_reviews[key])\n",
    "\n",
    "    for key in avg_review_length_collection:\n",
    "        training_data[key].append( (avg_review_length_collection[key] - 135.0 ))\n",
    "\n",
    "    for key in avg_deviation_member_rating:\n",
    "        if abs(avg_deviation_member_rating[key]) > 1.0:\n",
    "            training_data[key].append(5 * avg_deviation_member_rating[key] )\n",
    "        else:\n",
    "            training_data[key].append( avg_deviation_member_rating[key]  )\n",
    "\n",
    "    return training_data, mapping_training_id_user_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clustering\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 . K-Means\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spam_reviewers import *\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "training_data = construct_feature_vector()[0]\n",
    "final_training_data = []\n",
    "for value in training_data.values():\n",
    "\tfinal_training_data.append(value[1:])\n",
    "X = np.array(final_training_data)\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "predictions = kmeans.predict(X)\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "fake_reviewers = []\n",
    "for i in range(len(predictions)):\n",
    "\tif final_training_data[i][0] < 3:\n",
    "\t\tpredictions[i] = 1\n",
    "\tif predictions[i] == 0:\n",
    "\t\tcount_0 += 1\n",
    "\tif predictions[i] == 1:\n",
    "\t\tcount_1 += 1\n",
    "\tif predictions[i] == 0:\n",
    "\t\tfake_reviewers.append(training_data.keys()[i])\n",
    "\n",
    "print fake_reviewers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 . Self Organizing Maps\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "from numpy import (array, unravel_index, nditer, linalg, random, subtract,\n",
    "                   power, exp, pi, zeros, arange, outer, meshgrid, dot)\n",
    "from collections import defaultdict\n",
    "from warnings import warn\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Minimalistic implementation of the Self Organizing Maps (SOM).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def fast_norm(x):\n",
    "    \"\"\"Returns norm-2 of a 1-D numpy array.\n",
    "\n",
    "    * faster than linalg.norm in case of 1-D arrays (numpy 1.9.2rc1).\n",
    "    \"\"\"\n",
    "    return sqrt(dot(x, x.T))\n",
    "\n",
    "\n",
    "class MiniSom(object):\n",
    "    def __init__(self, x, y, input_len, sigma=1.0, learning_rate=0.5, decay_function=None, random_seed=None):\n",
    "        \"\"\"\n",
    "            Initializes a Self Organizing Maps.\n",
    "            x,y - dimensions of the SOM\n",
    "            input_len - number of the elements of the vectors in input\n",
    "            sigma - spread of the neighborhood function (Gaussian), needs to be adequate to the dimensions of the map.\n",
    "            (at the iteration t we have sigma(t) = sigma / (1 + t/T) where T is #num_iteration/2)\n",
    "            learning_rate - initial learning rate\n",
    "            (at the iteration t we have learning_rate(t) = learning_rate / (1 + t/T) where T is #num_iteration/2)\n",
    "            decay_function, function that reduces learning_rate and sigma at each iteration\n",
    "                            default function: lambda x,current_iteration,max_iter: x/(1+current_iteration/max_iter)\n",
    "            random_seed, random seed to use.\n",
    "        \"\"\"\n",
    "        if sigma >= x/2.0 or sigma >= y/2.0:\n",
    "            warn('Warning: sigma is too high for the dimension of the map.')\n",
    "        if random_seed:\n",
    "            self.random_generator = random.RandomState(random_seed)\n",
    "        else:\n",
    "            self.random_generator = random.RandomState(random_seed)\n",
    "        if decay_function:\n",
    "            self._decay_function = decay_function\n",
    "        else:\n",
    "            self._decay_function = lambda x, t, max_iter: x/(1+t/max_iter)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.sigma = sigma\n",
    "        self.weights = self.random_generator.rand(x,y,input_len)*2-1 # random initialization\n",
    "        for i in range(x):\n",
    "            for j in range(y):\n",
    "                self.weights[i,j] = self.weights[i,j] / fast_norm(self.weights[i,j]) # normalization\n",
    "        self.activation_map = zeros((x,y))\n",
    "        self.neigx = arange(x)\n",
    "        self.neigy = arange(y) # used to evaluate the neighborhood function\n",
    "        self.neighborhood = self.gaussian\n",
    "\n",
    "    def _activate(self, x):\n",
    "        \"\"\" Updates matrix activation_map, in this matrix the element i,j is the response of the neuron i,j to x \"\"\"\n",
    "        s = subtract(x, self.weights) # x - w\n",
    "        it = nditer(self.activation_map, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            self.activation_map[it.multi_index] = fast_norm(s[it.multi_index])  # || x - w ||\n",
    "            it.iternext()\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\" Returns the activation map to x \"\"\"\n",
    "        self._activate(x)\n",
    "        return self.activation_map\n",
    "\n",
    "    def gaussian(self, c, sigma):\n",
    "        \"\"\" Returns a Gaussian centered in c \"\"\"\n",
    "        d = 2*pi*sigma*sigma\n",
    "        ax = exp(-power(self.neigx-c[0], 2)/d)\n",
    "        ay = exp(-power(self.neigy-c[1], 2)/d)\n",
    "        return outer(ax, ay)  # the external product gives a matrix\n",
    "\n",
    "    def diff_gaussian(self, c, sigma):\n",
    "        \"\"\" Mexican hat centered in c (unused) \"\"\"\n",
    "        xx, yy = meshgrid(self.neigx, self.neigy)\n",
    "        p = power(xx-c[0], 2) + power(yy-c[1], 2)\n",
    "        d = 2*pi*sigma*sigma\n",
    "        return exp(-p/d)*(1-2/d*p)\n",
    "\n",
    "    def winner(self, x):\n",
    "        \"\"\" Computes the coordinates of the winning neuron for the sample x \"\"\"\n",
    "        self._activate(x)\n",
    "        return unravel_index(self.activation_map.argmin(), self.activation_map.shape)\n",
    "\n",
    "    def update(self, x, win, t):\n",
    "        \"\"\"\n",
    "            Updates the weights of the neurons.\n",
    "            x - current pattern to learn\n",
    "            win - position of the winning neuron for x (array or tuple).\n",
    "            t - iteration index\n",
    "        \"\"\"\n",
    "        eta = self._decay_function(self.learning_rate, t, self.T)\n",
    "        sig = self._decay_function(self.sigma, t, self.T) # sigma and learning rate decrease with the same rule\n",
    "        g = self.neighborhood(win, sig)*eta # improves the performances\n",
    "        it = nditer(g, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            # eta * neighborhood_function * (x-w)\n",
    "            self.weights[it.multi_index] += g[it.multi_index]*(x-self.weights[it.multi_index])\n",
    "            # normalization\n",
    "            self.weights[it.multi_index] = self.weights[it.multi_index] / fast_norm(self.weights[it.multi_index])\n",
    "            it.iternext()\n",
    "\n",
    "    def quantization(self, data):\n",
    "        \"\"\" Assigns a code book (weights vector of the winning neuron) to each sample in data. \"\"\"\n",
    "        q = zeros(data.shape)\n",
    "        for i, x in enumerate(data):\n",
    "            q[i] = self.weights[self.winner(x)]\n",
    "        return q\n",
    "\n",
    "    def random_weights_init(self, data):\n",
    "        \"\"\" Initializes the weights of the SOM picking random samples from data \"\"\"\n",
    "        it = nditer(self.activation_map, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            self.weights[it.multi_index] = data[self.random_generator.randint(len(data))]\n",
    "            self.weights[it.multi_index] = self.weights[it.multi_index]/fast_norm(self.weights[it.multi_index])\n",
    "            it.iternext()\n",
    "\n",
    "    def train_random(self, data, num_iteration):\n",
    "        \"\"\" Trains the SOM picking samples at random from data \"\"\"\n",
    "        self._init_T(num_iteration)\n",
    "        for iteration in range(num_iteration):\n",
    "            rand_i = self.random_generator.randint(len(data)) # pick a random sample\n",
    "            self.update(data[rand_i], self.winner(data[rand_i]), iteration)\n",
    "\n",
    "    def train_batch(self, data, num_iteration):\n",
    "        \"\"\" Trains using all the vectors in data sequentially \"\"\"\n",
    "        self._init_T(len(data)*num_iteration)\n",
    "        iteration = 0\n",
    "        while iteration < num_iteration:\n",
    "            idx = iteration % (len(data)-1)\n",
    "            self.update(data[idx], self.winner(data[idx]), iteration)\n",
    "            iteration += 1\n",
    "\n",
    "    def _init_T(self, num_iteration):\n",
    "        \"\"\" Initializes the parameter T needed to adjust the learning rate \"\"\"\n",
    "        self.T = num_iteration/2  # keeps the learning rate nearly constant for the last half of the iterations\n",
    "\n",
    "    def distance_map(self):\n",
    "        \"\"\" Returns the distance map of the weights.\n",
    "            Each cell is the normalised sum of the distances between a neuron and its neighbours.\n",
    "        \"\"\"\n",
    "        um = zeros((self.weights.shape[0], self.weights.shape[1]))\n",
    "        it = nditer(um, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            for ii in range(it.multi_index[0]-1, it.multi_index[0]+2):\n",
    "                for jj in range(it.multi_index[1]-1, it.multi_index[1]+2):\n",
    "                    if ii >= 0 and ii < self.weights.shape[0] and jj >= 0 and jj < self.weights.shape[1]:\n",
    "                        um[it.multi_index] += fast_norm(self.weights[ii, jj, :]-self.weights[it.multi_index])\n",
    "            it.iternext()\n",
    "        um = um/um.max()\n",
    "        return um\n",
    "\n",
    "    def activation_response(self, data):\n",
    "        \"\"\"\n",
    "            Returns a matrix where the element i,j is the number of times\n",
    "            that the neuron i,j have been winner.\n",
    "        \"\"\"\n",
    "        a = zeros((self.weights.shape[0], self.weights.shape[1]))\n",
    "        for x in data:\n",
    "            a[self.winner(x)] += 1\n",
    "        return a\n",
    "\n",
    "    def quantization_error(self, data):\n",
    "        \"\"\"\n",
    "            Returns the quantization error computed as the average distance between\n",
    "            each input sample and its best matching unit.\n",
    "        \"\"\"\n",
    "        error = 0\n",
    "        for x in data:\n",
    "            error += fast_norm(x-self.weights[self.winner(x)])\n",
    "        return error/len(data)\n",
    "\n",
    "    def win_map(self, data):\n",
    "        \"\"\"\n",
    "            Returns a dictionary wm where wm[(i,j)] is a list with all the patterns\n",
    "            that have been mapped in the position i,j.\n",
    "        \"\"\"\n",
    "        winmap = defaultdict(list)\n",
    "        for x in data:\n",
    "            winmap[self.winner(x)].append(x)\n",
    "        return winmap\n",
    "\n",
    "### unit tests\n",
    "from numpy.testing import assert_almost_equal, assert_array_almost_equal, assert_array_equal\n",
    "\n",
    "\n",
    "class TestMinisom:\n",
    "    def setup_method(self, method):\n",
    "        self.som = MiniSom(5, 5, 1)\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                assert_almost_equal(1.0, linalg.norm(self.som.weights[i,j]))  # checking weights normalization\n",
    "        self.som.weights = zeros((5, 5))  # fake weights\n",
    "        self.som.weights[2, 3] = 5.0\n",
    "        self.som.weights[1, 1] = 2.0\n",
    "\n",
    "    def test_decay_function(self):\n",
    "        assert self.som._decay_function(1., 2., 3.) == 1./(1.+2./3.)\n",
    "\n",
    "    def test_fast_norm(self):\n",
    "        assert fast_norm(array([1, 3])) == sqrt(1+9)\n",
    "\n",
    "    def test_gaussian(self):\n",
    "        bell = self.som.gaussian((2, 2), 1)\n",
    "        assert bell.max() == 1.0\n",
    "        assert bell.argmax() == 12  # unravel(12) = (2,2)\n",
    "\n",
    "    def test_win_map(self):\n",
    "        winners = self.som.win_map([5.0, 2.0])\n",
    "        assert winners[(2, 3)][0] == 5.0\n",
    "        assert winners[(1, 1)][0] == 2.0\n",
    "\n",
    "    def test_activation_reponse(self):\n",
    "        response = self.som.activation_response([5.0, 2.0])\n",
    "        assert response[2, 3] == 1\n",
    "        assert response[1, 1] == 1\n",
    "\n",
    "    def test_activate(self):\n",
    "        assert self.som.activate(5.0).argmin() == 13.0  # unravel(13) = (2,3)\n",
    "\n",
    "    def test_quantization_error(self):\n",
    "        self.som.quantization_error([5, 2]) == 0.0\n",
    "        self.som.quantization_error([4, 1]) == 0.5\n",
    "\n",
    "    def test_quantization(self):\n",
    "        q = self.som.quantization(array([4, 2]))\n",
    "        assert q[0] == 5.0\n",
    "        assert q[1] == 2.0\n",
    "\n",
    "    def test_random_seed(self):\n",
    "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        assert_array_almost_equal(som1.weights, som2.weights)  # same initialization\n",
    "        data = random.rand(100,2)\n",
    "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som1.train_random(data,10)\n",
    "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som2.train_random(data,10)\n",
    "        assert_array_almost_equal(som1.weights,som2.weights)  # same state after training\n",
    "\n",
    "    def test_train_batch(self):\n",
    "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        data = array([[4, 2], [3, 1]])\n",
    "        q1 = som.quantization_error(data)\n",
    "        som.train_batch(data, 10)\n",
    "        assert q1 > som.quantization_error(data)\n",
    "\n",
    "    def test_train_random(self):\n",
    "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        data = array([[4, 2], [3, 1]])\n",
    "        q1 = som.quantization_error(data)\n",
    "        som.train_random(data, 10)\n",
    "        assert q1 > som.quantization_error(data)\n",
    "\n",
    "    def test_random_weights_init(self):\n",
    "        som = MiniSom(2, 2, 2, random_seed=1)\n",
    "        som.random_weights_init(array([[1.0, .0]]))\n",
    "        for w in som.weights:\n",
    "            assert_array_equal(w[0], array([1.0, .0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from spam_reviewers import *\n",
    "X = np.array(construct_feature_vector()[0].values())\n",
    "mapping = construct_feature_vector()[1]\n",
    "\n",
    "# Training the SOM\n",
    "# We will use MiniSom 1.0\n",
    "from minisom import MiniSom\n",
    "som = MiniSom(x = 10, y = 10, input_len = 5, sigma = 1.0, learning_rate = 0.5)\n",
    "som.random_weights_init(X)\n",
    "som.train_random(data = X, num_iteration = 100000)\n",
    "\n",
    "\n",
    "som_distance = som.distance_map().T\n",
    "\n",
    "mappings = som.win_map(X)\n",
    "print('size of the SOM map', len(mappings))\n",
    "text_file = open(\"mappings_individual.txt\", \"w\")\n",
    "text_file.write(\"winning map is %s\" % mappings)\n",
    "text_file.close()\n",
    "\n",
    "# List of fake review user IDs\n",
    "faker_list = []\n",
    "faker_id = []\n",
    "for i in xrange(len(som_distance)):\n",
    "\tfor j in xrange(len(som_distance[0])):\n",
    "\t\tif som_distance[i][j] > 0.8:\n",
    "\t\t\tfake_reviewers = mappings[(i,j)]\n",
    "\t\t\tfor user in fake_reviewers:\n",
    "\t\t\t\tfaker_id.append(user[0])\n",
    "\t\t\t\tfaker_list.append(mapping[user[0]])\n",
    "print('list of fake user IDs is\\n', faker_list)\n",
    "\n",
    "# Visualizing the results (SOM in a 2-D plot)\n",
    "from pylab import bone, pcolor, colorbar, plot, show\n",
    "bone() # creates a white window\n",
    "pcolor(som.distance_map().T)\n",
    "colorbar()\n",
    "markers = ['o', 's']\n",
    "colors = ['r', 'g']\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Result Evaluation & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You may use markdown like this\n",
    "\n",
    "\n",
    "```python\n",
    "print \"Hello World\"\n",
    "```\n",
    "\n",
    "## You may use LaTeX equation like this\n",
    "\n",
    "$$e^x=\\sum_{i=0}^\\infty \\frac{1}{i!}x^i$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You may use table as html like this\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <td> row head 1 \n",
    "            <td> row head 2\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td> row body 1\n",
    "            <td> row body 2\n",
    "        </tr>   \n",
    "    </tbody>    \n",
    "</table>\n",
    "\n",
    "## You may insert photo like this\n",
    "\n",
    "<img src=\"./images/test.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] http://liu.cs.uic.edu/download/data/\n",
    "\n",
    "[2] https://www.cs.uic.edu/~liub/publications/WWW-2012-group-spam-camera-final.pdf\n",
    "\n",
    "[3] https://www.hindawi.com/journals/mpe/2016/4935792/\n",
    "\n",
    "[4] https://www.cs.uic.edu/~liub/FBS/fake-reviews.html\n",
    "\n",
    "[5] http://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "[6] http://www.bbc.com/news/technology-22166606\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {delete before submission} Proposal -- You may need to refer to this paragraph\n",
    "\n",
    "Prior works on opinion spam focused on detecting fake reviews and individual fake reviewers. However, a fake reviewer group (a group of reviewers who work collaboratively to write fake reviews) is even more damaging as they can take total control of the sentiment on the target product due to its size. Therefore, we are going to follow our base paper trying to identify fake reviews by clustering fake spammers into groups, which are also called group spammers[1]. Group spamming refers to a group of reviewers writing fake reviews together to promote or to demote some target products. The base paper[1] we choose has done experiments that show it is hard to detect spammer groups using review content features or even indicators for detecting abnormal behaviors of individual spammers because a group has more manpower to post reviews and thus, each member may no longer appear to behave abnormally. A group of reviewers refers to a set of reviewer-ids. The actual reviewers behind the ids could be a single person with multiple ids (sockpuppet), multiple persons, or a combination of both.\n",
    "\n",
    "\n",
    "Therefore, we will also implement a relation-based model “GSRank” described in [2] using an Artificial Neural Network [5] as the following figure1. The GS rank algorithm is an unsupervised iterative algorithm that works differently from the traditional supervised learning approach to spam detection. The paper [2] has concluded after experiments that “GSRank” performs better than the state-of-the-art supervised classification, regression, and learning to rank algorithms. Basically, we also follow this paper [2] to build a more effective model which can consider the inter-relationship among products, groups, and group members in computing group spamicity. In other words, we will try to reimplement the paper from scratch. In conclusion, after getting two different result as we mentioned above, we will evaluate with Precision, Recall and NDCG.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {delete before submission}  Requirement\n",
    "\n",
    "\n",
    "\n",
    "Your main deliverable is your project notebook. This will act as both a written report plus a walkthrough of your code. This is the critical piece that will document and detail your project experience. We expect your project notebook to tell us the story of your project -- from initial question and data collection, to initial exploratory data analysis, perhaps to a revised question, to analyses, visualizations, and key takeaways.\n",
    "\n",
    "\n",
    "Note that we do not want to see a completely raw, moment-by-moment accounting of your project (include all 99 missteps and dead-ends); rather, you should carefully put together your final project notebook for submission that captures the key steps along the way.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cs670]",
   "language": "python",
   "name": "conda-env-cs670-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
